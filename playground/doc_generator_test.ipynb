{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Populate Pinecone Document Store with Test Case Documents"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ac9e9f500c62e007"
  },
  {
   "cell_type": "markdown",
   "source": [
    "To delete all records u need to `pip install \"pinecone[grpc]\"` and run the following code."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f88dbfb9166fe5db"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": ""
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pinecone.grpc import PineconeGRPC as Pinecone\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "pinecone_api_key = os.getenv(\"PINECONE_API_KEY\")\n",
    "\n",
    "pc = Pinecone(api_key=pinecone_api_key)\n",
    "index = pc.Index(\"default\")\n",
    "\n",
    "index.delete(delete_all=True, namespace='default')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-19T14:25:19.525163Z",
     "start_time": "2024-09-19T14:25:18.450197Z"
    }
   },
   "id": "31079a5e4221fea4",
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "Initialize Pinecone Document Store"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ba1f1a8bb47c9411"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from haystack_integrations.document_stores.pinecone import PineconeDocumentStore\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "document_store = PineconeDocumentStore(\n",
    "\t\tindex=\"default\",\n",
    "\t\tnamespace=\"default\",\n",
    "\t\tdimension=384,\n",
    "  \tmetric=\"cosine\",\n",
    "  \tspec={\"serverless\": {\"region\": \"us-east-1\", \"cloud\": \"aws\"}}\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-19T14:25:25.942335Z",
     "start_time": "2024-09-19T14:25:23.959602Z"
    }
   },
   "id": "a745131d322d28",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "Prepare pipeline components"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7422ae78ff5d8521"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from haystack.components.routers import FileTypeRouter\n",
    "from haystack.components.converters import TextFileToDocument, MarkdownToDocument, PyPDFToDocument \n",
    "from haystack.components.joiners import DocumentJoiner\n",
    "from haystack.components.preprocessors import DocumentSplitter, DocumentCleaner\n",
    "from haystack.components.embedders import SentenceTransformersDocumentEmbedder\n",
    "from haystack.components.writers import DocumentWriter\n",
    "\n",
    "file_type_router = FileTypeRouter(mime_types=[\"text/plain\", \"application/pdf\", \"text/markdown\"])\n",
    "text_file_converter = TextFileToDocument()\n",
    "markdown_converter = MarkdownToDocument()\n",
    "pdf_converter = PyPDFToDocument()\n",
    "document_joiner = DocumentJoiner()\n",
    "document_cleaner = DocumentCleaner()\n",
    "document_splitter = DocumentSplitter(split_by=\"word\", split_length=150, split_overlap=50)\n",
    "document_embedder = SentenceTransformersDocumentEmbedder(model=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "document_writer = DocumentWriter(document_store)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-19T14:25:29.153295Z",
     "start_time": "2024-09-19T14:25:27.521319Z"
    }
   },
   "id": "5142cfb8161abc1",
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create a pipeline to populate the Pinecone Document Store with test case documents"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3bfe34af87648a2a"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "<haystack.core.pipeline.pipeline.Pipeline object at 0x0000026CBEEE5EE0>\nðŸš… Components\n  - file_type_router: FileTypeRouter\n  - text_file_converter: TextFileToDocument\n  - markdown_converter: MarkdownToDocument\n  - pypdf_converter: PyPDFToDocument\n  - document_joiner: DocumentJoiner\n  - document_cleaner: DocumentCleaner\n  - document_splitter: DocumentSplitter\n  - document_embedder: SentenceTransformersDocumentEmbedder\n  - document_writer: DocumentWriter\nðŸ›¤ï¸ Connections\n  - file_type_router.text/plain -> text_file_converter.sources (List[Path])\n  - file_type_router.application/pdf -> pypdf_converter.sources (List[Path])\n  - file_type_router.text/markdown -> markdown_converter.sources (List[Path])\n  - text_file_converter.documents -> document_joiner.documents (List[Document])\n  - markdown_converter.documents -> document_joiner.documents (List[Document])\n  - pypdf_converter.documents -> document_joiner.documents (List[Document])\n  - document_joiner.documents -> document_cleaner.documents (List[Document])\n  - document_cleaner.documents -> document_splitter.documents (List[Document])\n  - document_splitter.documents -> document_embedder.documents (List[Document])\n  - document_embedder.documents -> document_writer.documents (List[Document])"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from haystack import Pipeline\n",
    "\n",
    "preprocessing_pipeline = Pipeline()\n",
    "preprocessing_pipeline.add_component(instance=file_type_router, name=\"file_type_router\")\n",
    "preprocessing_pipeline.add_component(instance=text_file_converter, name=\"text_file_converter\")\n",
    "preprocessing_pipeline.add_component(instance=markdown_converter, name=\"markdown_converter\")\n",
    "preprocessing_pipeline.add_component(instance=pdf_converter, name=\"pypdf_converter\")\n",
    "preprocessing_pipeline.add_component(instance=document_joiner, name=\"document_joiner\")\n",
    "preprocessing_pipeline.add_component(instance=document_cleaner, name=\"document_cleaner\")\n",
    "preprocessing_pipeline.add_component(instance=document_splitter, name=\"document_splitter\")\n",
    "preprocessing_pipeline.add_component(instance=document_embedder, name=\"document_embedder\")\n",
    "preprocessing_pipeline.add_component(instance=document_writer, name=\"document_writer\")\n",
    "\n",
    "preprocessing_pipeline.connect(\"file_type_router.text/plain\", \"text_file_converter.sources\")\n",
    "preprocessing_pipeline.connect(\"file_type_router.application/pdf\", \"pypdf_converter.sources\")\n",
    "preprocessing_pipeline.connect(\"file_type_router.text/markdown\", \"markdown_converter.sources\")\n",
    "preprocessing_pipeline.connect(\"text_file_converter\", \"document_joiner\")\n",
    "preprocessing_pipeline.connect(\"pypdf_converter\", \"document_joiner\")\n",
    "preprocessing_pipeline.connect(\"markdown_converter\", \"document_joiner\")\n",
    "preprocessing_pipeline.connect(\"document_joiner\", \"document_cleaner\")\n",
    "preprocessing_pipeline.connect(\"document_cleaner\", \"document_splitter\")\n",
    "preprocessing_pipeline.connect(\"document_splitter\", \"document_embedder\")\n",
    "preprocessing_pipeline.connect(\"document_embedder\", \"document_writer\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-19T14:25:32.242330Z",
     "start_time": "2024-09-19T14:25:32.235843Z"
    }
   },
   "id": "f0df56147ec0fad8",
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "Run the pipeline"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c8fddbeb5665f26"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\filip\\Desktop\\venvs\\wireenv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Converting markdown files to Documents: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 1599.81it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": "Batches:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5cf5370449b7404ab7f6a27a95b55ebf"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Document 136bcc65efa842bf4dab45cc51c0b89df20d582eb2c965c08134364c020bc1ac has metadata fields with unsupported types: ['_split_overlap']. Only str, int, bool, and List[str] are supported. The values of these fields will be discarded.\n",
      "Document a6c0e5304630bcd37e2dad1fff7ab046d0d9965e26384b08d134d08a3bf009cd has metadata fields with unsupported types: ['_split_overlap']. Only str, int, bool, and List[str] are supported. The values of these fields will be discarded.\n"
     ]
    },
    {
     "data": {
      "text/plain": "Upserted vectors:   0%|          | 0/25 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0eaa8f34ad6b48faa387f1fb280f8373"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "{'document_writer': {'documents_written': 25}}"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "preprocessing_pipeline.run({\"file_type_router\": {\"sources\": list(Path(Path('./Data/Test_Case')).glob(\"**/*\"))}})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-19T14:25:42.647843Z",
     "start_time": "2024-09-19T14:25:38.911033Z"
    }
   },
   "id": "7a1c47cb85342d86",
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test RAG with Pinecone Document Store"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9eae0c4f1384253e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Restart the kernel and run the following code to test the RAG pipeline with the populated Pinecone Document Store."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "75a635b2d20f0142"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from haystack_integrations.document_stores.pinecone import PineconeDocumentStore\n",
    "\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "document_store = PineconeDocumentStore(\n",
    "    index=\"default\",\n",
    "    namespace=\"default\",\n",
    "    dimension=384,\n",
    "  \tmetric=\"cosine\",\n",
    "  \tspec={\"serverless\": {\"region\": \"us-east-1\", \"cloud\": \"aws\"}}\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-19T14:27:27.230541Z",
     "start_time": "2024-09-19T14:27:24.879939Z"
    }
   },
   "id": "6bfc12597e46077",
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create pipeline to run a query"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "58ebaae516e5343c"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "<haystack.core.pipeline.pipeline.Pipeline object at 0x0000012E37142A20>\nðŸš… Components\n  - text_embedder: SentenceTransformersTextEmbedder\n  - retriever: PineconeEmbeddingRetriever\n  - prompt_builder: PromptBuilder\n  - llm: OpenAIGenerator\n  - answer_builder: AnswerBuilder\nðŸ›¤ï¸ Connections\n  - text_embedder.embedding -> retriever.query_embedding (List[float])\n  - retriever.documents -> prompt_builder.documents (List[Document])\n  - retriever.documents -> answer_builder.documents (List[Document])\n  - prompt_builder.prompt -> llm.prompt (str)\n  - llm.replies -> answer_builder.replies (List[str])"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from haystack.components.embedders import SentenceTransformersDocumentEmbedder, SentenceTransformersTextEmbedder\n",
    "from haystack_integrations.components.retrievers.pinecone import PineconeEmbeddingRetriever\n",
    "from haystack.components.generators import OpenAIGenerator\n",
    "from haystack.components.builders.answer_builder import AnswerBuilder\n",
    "from haystack.components.builders.prompt_builder import PromptBuilder\n",
    "from haystack import Pipeline\n",
    "\n",
    "template = \"\"\"\n",
    "    Given these documents, answer the question.\\nDocuments:\n",
    "    {% for doc in documents %}\n",
    "        {{ doc.content }}\n",
    "    {% endfor %}\n",
    "\n",
    "    \\nQuestion: {{question}}\n",
    "    \\nAnswer:\n",
    "\"\"\"\n",
    "\n",
    "document_embedder = SentenceTransformersDocumentEmbedder(model=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "text_embedder = SentenceTransformersTextEmbedder(model=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "retriever = PineconeEmbeddingRetriever(document_store=document_store)\n",
    "generator = OpenAIGenerator()\n",
    "answer_builder = AnswerBuilder()\n",
    "prompt_builder = PromptBuilder(template=template)\n",
    "\n",
    "rag_pipeline = Pipeline()\n",
    "rag_pipeline.add_component(\"text_embedder\", text_embedder)\n",
    "rag_pipeline.add_component(\"retriever\", retriever)\n",
    "rag_pipeline.add_component(\"prompt_builder\", prompt_builder)\n",
    "rag_pipeline.add_component(\"llm\", generator)\n",
    "rag_pipeline.add_component(\"answer_builder\", answer_builder)\n",
    "\n",
    "rag_pipeline.connect(\"text_embedder.embedding\", \"retriever.query_embedding\")\n",
    "rag_pipeline.connect(\"retriever\", \"prompt_builder.documents\")\n",
    "rag_pipeline.connect(\"prompt_builder\", \"llm\")\n",
    "rag_pipeline.connect(\"llm.replies\", \"answer_builder.replies\")\n",
    "rag_pipeline.connect(\"retriever\", \"answer_builder.documents\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-19T14:28:02.990363Z",
     "start_time": "2024-09-19T14:28:01.795209Z"
    }
   },
   "id": "1b38c8ef5aacfe0",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "Run the pipeline with a query"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f7c6c62a9fa033a1"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Batches:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0e90269606f74c45a52321cc38d1065f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate full documentation of DataAnalyzer project\n",
      "# DataAnalyzer Project Documentation\n",
      "\n",
      "## Module Name: DataAnalyzer\n",
      "## Version: 2.5.0\n",
      "## Author: Tech Solutions\n",
      "\n",
      "## Overview:\n",
      "The DataAnalyzer project provides a fast and optimized approach to data analysis, suitable for small datasets in personal and academic projects. It offers various functions for processing datasets and generating basic statistics and visualizations.\n",
      "\n",
      "## Functions:\n",
      "1. **process_data(dataset: list) -> dict**\n",
      "   This function processes the dataset and returns basic statistics such as:\n",
      "   - Mean\n",
      "   - Mode\n",
      "   - Variance\n",
      "   Usage Example:\n",
      "   ```python\n",
      "   from DataAnalyzer import process_data\n",
      "   dataset = [2, 8, 22, 18, 25]\n",
      "   stats = process_data(dataset)\n",
      "   ```\n",
      "\n",
      "2. **graph_data(dataset: list, chart: str = 'scatter') -> None**\n",
      "   This function generates a chart based on the dataset. The default chart type is 'scatter'.\n",
      "   Example:\n",
      "   ```python\n",
      "   from DataAnalyzer import graph_data\n",
      "   dataset = [2, 8, 22, 18, 25]\n",
      "   graph_data(dataset, 'scatter')\n",
      "   ```\n",
      "\n",
      "## Installation:\n",
      "Install the DataAnalyzer package via pip:\n",
      "```\n",
      "pip install analyzer-package\n",
      "```\n",
      "\n",
      "## Dependencies:\n",
      "- Python 3.6+\n",
      "- pandas\n",
      "- seaborn\n",
      "\n",
      "## Notes:\n",
      "- The function `process_data` replaces `analyze_data`.\n",
      "- Default chart type for `graph_data` changed from `bar` to `scatter`.\n",
      "- Added support for new chart types: 'heatmap' and 'pie'.\n",
      "- Improved performance for small datasets.\n",
      "[Document(id=5858b0617db9c1b474fa484f0a3bec8317fded4cfcdae84552bbee3249faf4d0, content: 'Code Documentation for DataAnalyzer.py\n",
      "Module Name: DataAnalyzer\n",
      "Version: 1.0.0\n",
      "Author: Tech Solutio...', meta: {'_split_overlap': [], 'file_path': 'Data\\\\Test_Case\\\\1.pdf', 'page_number': 1, 'source_id': '1416f0a216fd69c6e9a9028316744a042dc4663e8c0873b02eca4661feb987a2', 'split_id': 0, 'split_idx_start': 0}, score: 0.507804513, embedding: vector of size 384), Document(id=29a7dae24a4df16e8953fc75558c569e82cd47e3c6919d810fe9e81d12287724, content: 'DataAnalyzer Version: 2.5.0\n",
      "Author: Tech Innovations Overview The DataAnalyzer package provides a fa...', meta: {'_split_overlap': [], 'file_path': 'Data\\\\Test_Case\\\\1.md', 'page_number': 1, 'source_id': 'fa6b471b2169c5cf6b138e74e2f334fba71150573d6d513d052a67560a947eac', 'split_id': 0, 'split_idx_start': 0}, score: 0.467273474, embedding: vector of size 384), Document(id=09f41e90e86ba0d5f2ea54e6307656d06620c27481d9ab058bccacc72152c411, content: 'Meeting notes - The function `process_data` replaces `analyze_data`.\n",
      "- Default chart type for `graph...', meta: {'_split_overlap': [], 'file_path': 'Data\\\\Test_Case\\\\1.txt', 'page_number': 1, 'source_id': '49d7075b862b3ece8b6f123d4fd9c7a980ddfd6a507f7f470630e9f5cd2116d3', 'split_id': 0, 'split_idx_start': 0}, score: 0.412864298, embedding: vector of size 384), Document(id=88784b322a1bc9eb5bdac32739fea8e3ec38a0db2ab933759c93980e113f61e1, content: '# Patch Notes - reverse_string was renamed to analyze_text.\n",
      "- analyze_text now performs text analysi...', meta: {'_split_overlap': [], 'file_path': 'Data\\\\Test_Case\\\\5.txt', 'page_number': 1, 'source_id': '242e8ac8c27e41a76f07befe962ce4cbee2d6f5e503502927bc4126246e9e90e', 'split_id': 0, 'split_idx_start': 0}, score: 0.326104552, embedding: vector of size 384), Document(id=3aca361c8962d6556c966b5d8576d91034778781053db20e93abd0f2a52fd7d6, content: 'README Project Description This Ruby script is designed for text analysis, not basic string reversal...', meta: {'_split_overlap': [], 'file_path': 'Data\\\\Test_Case\\\\5.md', 'page_number': 1, 'source_id': '8fc343842065a5149f581ff978730fe914a91bb430ec0479c23d31e716217cfc', 'split_id': 0, 'split_idx_start': 0}, score: 0.285757244, embedding: vector of size 384), Document(id=4512a83362922d5f35826754e5a79ec748198457d02fa01950f72d2ce953ed2a, content: '# Python Code Documentation\n",
      "## Overview\n",
      "This document outlines the structure of a Python program des...', meta: {'_split_overlap': [], 'file_path': 'Data\\\\Test_Case\\\\3.pdf', 'page_number': 1, 'source_id': 'a388a2b8d91d07051ee48069f9a28f48f16e0729876589b8154584cfb60abeb7', 'split_id': 0, 'split_idx_start': 0}, score: 0.194215313, embedding: vector of size 384), Document(id=43d89de410abee85f1612ac4cb5847334e4285d77ca9cdc14f6ea232f676b10b, content: 'README Project Description This Python script is an interactive command-line tool designed for proce...', meta: {'_split_overlap': [], 'file_path': 'Data\\\\Test_Case\\\\3.md', 'page_number': 1, 'source_id': 'f19225d52b2230e510de7cb747efed97f2d1335dfd2ae2a017f0c3c969213f5c', 'split_id': 0, 'split_idx_start': 0}, score: 0.183664158, embedding: vector of size 384), Document(id=317094e95a461034dab2952073d52e1365e624a8cd11948f5f150fbbe51980ae, content: 'README Project Description This Java project is designed to demonstrate basic input/output handling....', meta: {'_split_overlap': [], 'file_path': 'Data\\\\Test_Case\\\\2.md', 'page_number': 1, 'source_id': 'ebdee72975da0e6d66faaf7ab0c3e275cf59e74438c5a7461c3692734ca77596', 'split_id': 0, 'split_idx_start': 0}, score: 0.168326199, embedding: vector of size 384), Document(id=5bdb1283969d71fda11f1b094bb9419b462a0784e0dcabe5ba095c820e282068, content: '# Patch Notes - Chat functionality replaced with file transfer.\n",
      "- Message handling functions were ch...', meta: {'_split_overlap': [], 'file_path': 'Data\\\\Test_Case\\\\7.txt', 'page_number': 1, 'source_id': 'ee0e31625150eda2c086e5f28f6030c85733c825a2d3f5622f418bde57f005e3', 'split_id': 0, 'split_idx_start': 0}, score: 0.166660532, embedding: vector of size 384), Document(id=45f2e3d9ca3beffe1ba8931fcc3f21a28079adad00d3773b0d70c92bbc1c0585, content: '# Patch Notes - substraction funtion added\n",
      "- helperFunction output format is changed', meta: {'_split_overlap': [], 'file_path': 'Data\\\\Test_Case\\\\4.txt', 'page_number': 1, 'source_id': '979196cb4a7f55e85d2bfefb6acbdf13b7c17b1661d84575c9d6bcc76a010221', 'split_id': 0, 'split_idx_start': 0}, score: 0.161695659, embedding: vector of size 384)]\n"
     ]
    }
   ],
   "source": [
    "query = \"Generate full documentation of DataAnalyzer project\"\n",
    "result = rag_pipeline.run({\n",
    "    \"text_embedder\": {\"text\": query},\n",
    "    \"prompt_builder\": {\"question\": query},\n",
    "    \"answer_builder\": {\"query\": query}\n",
    "})\n",
    "\n",
    "print(result['answer_builder']['answers'][0].query)\n",
    "print(result['answer_builder']['answers'][0].data)\n",
    "print(result['answer_builder']['answers'][0].documents)\n",
    "\n",
    "with open(\"./Data/Outputs/output.md\", \"w\") as f:\n",
    "    f.write(result['answer_builder']['answers'][0].data)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-19T14:28:48.173100Z",
     "start_time": "2024-09-19T14:28:42.996334Z"
    }
   },
   "id": "d066fdea1ddd46d3",
   "execution_count": 4
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
