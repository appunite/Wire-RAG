{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T15:27:38.578571Z",
     "start_time": "2024-10-01T15:27:37.969462Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from utils.url_scraper import extract_content_and_metadata, DATE_FORMATS, DATE_PATTERNS\n",
    "\n",
    "x = extract_content_and_metadata(\"https://docs.wire.com/how-to/install/infrastructure-configuration.html#i-have-a-team-larger-than-500-users\", DATE_FORMATS, DATE_PATTERNS)"
   ],
   "id": "4bdb15a008c91503",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T15:28:30.531655Z",
     "start_time": "2024-10-01T15:28:30.526752Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for i in x[:1]:\n",
    "    print(f\"{i['metadata']['headline']}\\n{i['content']}\")\n",
    "    \n",
    "print(f\"\\n{x[0]['content']}\")"
   ],
   "id": "99d74ad23f492b70",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Infrastructure configuration options\n",
      " This contains instructions to configure specific aspects of your production setup depending on your needs. Depending on your use-case and requirements, you may need to\n",
      "configure none, or only a subset of the following sections.\n",
      "\n",
      " This contains instructions to configure specific aspects of your production setup depending on your needs. Depending on your use-case and requirements, you may need to\n",
      "configure none, or only a subset of the following sections.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T15:53:47.264130Z",
     "start_time": "2024-10-01T15:53:46.743203Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import aiohttp\n",
    "import asyncio\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import re\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "import requests\n",
    "from collections import defaultdict\n",
    "\n",
    "def requests_retry_session(\n",
    "        retries=3,\n",
    "        backoff_factor=0.3,\n",
    "        status_forcelist=(500, 502, 504),\n",
    "        session=None,\n",
    "):\n",
    "    session = session or requests.Session()\n",
    "    retry = Retry(\n",
    "        total=retries,\n",
    "        read=retries,\n",
    "        connect=retries,\n",
    "        backoff_factor=backoff_factor,\n",
    "        status_forcelist=status_forcelist,\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retry)\n",
    "    session.mount('http://', adapter)\n",
    "    session.mount('https://', adapter)\n",
    "    return session\n",
    "\n",
    "my_url = \"https://docs.wire.com/how-to/install/infrastructure-configuration.html#i-have-a-team-larger-than-500-users\"\n",
    "session = requests_retry_session()\n",
    "response = session.get(my_url, timeout=20)\n",
    "\n",
    "# Check if request was successful\n",
    "response.raise_for_status()\n",
    "\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "content_by_headline = defaultdict(str)\n",
    "current_header = None\n",
    "for element in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p', 'pre']):\n",
    "    if element.name.startswith('h'):\n",
    "        # Headline\n",
    "        print(f\"$$$ {element.name} $$$ {element.get_text(strip=False)}\")\n",
    "        current_header = element.get_text(strip=True)\n",
    "        current_header = current_header.replace(\"\\n\", \"\")\n",
    "        current_header = re.sub(r\"\\s+\", \" \", current_header)\n",
    "        current_header = re.sub(r'[^\\x00-\\x7F]+', '', current_header)\n",
    "\n",
    "    elif element.name in ['p', 'pre'] and current_header:\n",
    "        # Append the text under the last seen headline\n",
    "        print(f\"^^^ {element.name} ^^^ {element.get_text(strip=False)}\")\n",
    "        if element.name == 'pre':\n",
    "            new_content = element.get_text(strip=False)\n",
    "        else:\n",
    "            new_content = re.sub(r'\\s+', ' ', element.get_text(strip=False)).replace(\"\\n\", \" \")\n",
    "        content_by_headline[current_header] += f\"\\n{new_content}\"\n",
    "            "
   ],
   "id": "540412bcd5ea269",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$$$ h1 $$$ Infrastructure configuration options\n",
      "^^^ p ^^^ This contains instructions to configure specific aspects of your production setup depending on your needs.\n",
      "^^^ p ^^^ Depending on your use-case and requirements, you may need to\n",
      "configure none, or only a subset of the following sections.\n",
      "$$$ h2 $$$ Redirect some traffic through a http(s) proxy\n",
      "^^^ p ^^^ In case you wish to use http(s) proxies, you can add a configuration like this to the wire-server services in question:\n",
      "^^^ p ^^^ Assuming your proxy can be reached from within Kubernetes at http://proxy:8080, add the following for each affected service (e.g. gundeck) to your Helm overrides in values/wire-server/values.yaml :\n",
      "^^^ pre ^^^ gundeck:\n",
      "  # ...\n",
      "  config:\n",
      "    # ...\n",
      "    proxy:\n",
      "      httpProxy: \"http://proxy:8080\"\n",
      "      httpsProxy: \"http://proxy:8080\"\n",
      "      noProxyList:\n",
      "        - \"localhost\"\n",
      "        - \"127.0.0.1\"\n",
      "        - \"10.0.0.0/8\"\n",
      "        - \"elasticsearch-external\"\n",
      "        - \"cassandra-external\"\n",
      "        - \"redis-ephemeral\"\n",
      "        - \"fake-aws-sqs\"\n",
      "        - \"fake-aws-dynamodb\"\n",
      "        - \"fake-aws-sns\"\n",
      "        - \"brig\"\n",
      "        - \"cargohold\"\n",
      "        - \"galley\"\n",
      "        - \"gundeck\"\n",
      "        - \"proxy\"\n",
      "        - \"spar\"\n",
      "        - \"federator\"\n",
      "        - \"cannon\"\n",
      "        - \"cannon-0.cannon.default\"\n",
      "        - \"cannon-1.cannon.default\"\n",
      "        - \"cannon-2.cannon.default\"\n",
      "\n",
      "^^^ p ^^^ Depending on your setup, you may need to repeat this for the other services like brig as well.\n",
      "$$$ h2 $$$ Enable push notifications using the public appstore / playstore mobile Wire clients\n",
      "^^^ p ^^^ You need to get in touch with us. Please talk to sales or customer support - see https://wire.com\n",
      "^^^ p ^^^ If a contract agreement has been reached, we can set up a separate AWS account for you containing the necessary AWS SQS/SNS setup to route push notifications through to the mobile apps. We will then forward some configuration / access credentials that looks like:\n",
      "^^^ pre ^^^ gundeck:\n",
      "  config:\n",
      "    aws:\n",
      "      account: \"<REDACTED>\"\n",
      "      arnEnv: \"<REDACTED>\"\n",
      "      queueName: \"<REDACTED>-gundeck-events\"\n",
      "      region: \"<REDACTED>\"\n",
      "      snsEndpoint: \"https://sns.<REDACTED>.amazonaws.com\"\n",
      "      sqsEndpoint: \"https://sqs.<REDACTED>.amazonaws.com\"\n",
      "  secrets:\n",
      "    awsKeyId: \"<REDACTED>\"\n",
      "    awsSecretKey: \"<REDACTED>\"\n",
      "\n",
      "^^^ p ^^^ To make use of those, first test the credentials are correct, e.g. using the aws command-line tool (for more information on how to configure credentials, please refer to the official docs):\n",
      "^^^ pre ^^^ AWS_REGION=<region>\n",
      "AWS_ACCESS_KEY_ID=<...>\n",
      "AWS_SECRET_ACCESS_KEY=<...>\n",
      "ENV=<environment> #e.g staging\n",
      "\n",
      "aws sqs get-queue-url --queue-name \"$ENV-gundeck-events\"\n",
      "\n",
      "^^^ p ^^^ You should get a result like this:\n",
      "^^^ pre ^^^ {\n",
      "    \"QueueUrl\": \"https://<region>.queue.amazonaws.com/<aws-account-id>/<environment>-gundeck-events\"\n",
      "}\n",
      "\n",
      "^^^ p ^^^ Then add them to your gundeck configuration overrides.\n",
      "^^^ p ^^^ Keys below gundeck.config belong into values/wire-server/values.yaml:\n",
      "^^^ pre ^^^ gundeck:\n",
      "  # ...\n",
      "  config:\n",
      "    aws:\n",
      "      queueName: # e.g. staging-gundeck-events\n",
      "      account: # <aws-account-id>, e.g. 123456789\n",
      "      region: # e.g. eu-central-1\n",
      "      snsEndpoint: # e.g. https://sns.eu-central-1.amazonaws.com\n",
      "      sqsEndpoint: # e.g. https://sqs.eu-central-1.amazonaws.com\n",
      "      arnEnv: # e.g. staging - this must match the environment name (first part of queueName)\n",
      "\n",
      "^^^ p ^^^ Keys below gundeck.secrets belong into values/wire-server/secrets.yaml:\n",
      "^^^ pre ^^^ gundeck:\n",
      "  # ...\n",
      "  secrets:\n",
      "    awsKeyId: CHANGE-ME\n",
      "    awsSecretKey: CHANGE-ME\n",
      "\n",
      "^^^ p ^^^ After making this change and applying it to gundeck (ensure gundeck pods have restarted to make use of the updated configuration - that should happen automatically), make sure to reset the push token on any mobile devices that you may have in use.\n",
      "^^^ p ^^^ Next, if you want, you can stop using the fake-aws-sns pods in case you ran them before:\n",
      "^^^ pre ^^^ # inside override values/fake-aws/values.yaml\n",
      "fake-aws-sns:\n",
      "  enabled: false\n",
      "\n",
      "$$$ h2 $$$ Controlling the speed of websocket draining during cannon pod replacement\n",
      "^^^ p ^^^ The ‘cannon’ component is responsible for persistent websocket connections.\n",
      "Normally the default options would slowly and gracefully drain active websocket\n",
      "connections over a maximum of (amount of cannon replicas * 30 seconds) during\n",
      "the deployment of a new wire-server version. This will lead to a very brief\n",
      "interruption for Wire clients when their client has to re-connect on the\n",
      "websocket.\n",
      "^^^ p ^^^ You’re not expected to need to change these settings.\n",
      "^^^ p ^^^ The following options are only relevant during the restart of cannon itself.\n",
      "During a restart of nginz or ingress-controller, all websockets will get\n",
      "severed. If this is to be avoided, see section Separate incoming websocket network traffic from the rest of the https traffic\n",
      "^^^ p ^^^ drainOpts: Drain websockets in a controlled fashion when cannon receives a\n",
      "SIGTERM or SIGINT (this happens when a pod is terminated e.g. during rollout\n",
      "of a new version). Instead of waiting for connections to close on their own,\n",
      "the websockets are now severed at a controlled pace. This allows for quicker\n",
      "rollouts of new versions.\n",
      "^^^ p ^^^ There is no way to entirely disable this behaviour, two extreme examples below\n",
      "^^^ p ^^^ the quickest way to kill cannon is to set gracePeriodSeconds: 1 and\n",
      "minBatchSize: 100000 which would sever all connections immediately; but it’s\n",
      "not recommended as you could DDoS yourself by forcing all active clients to\n",
      "reconnect at the same time. With this, cannon pod replacement takes only 1\n",
      "second per pod.\n",
      "^^^ p ^^^ the slowest way to roll out a new version of cannon without severing websocket\n",
      "connections for a long time is to set minBatchSize: 1,\n",
      "millisecondsBetweenBatches: 86400000 and gracePeriodSeconds: 86400\n",
      "which would lead to one single websocket connection being closed immediately,\n",
      "and all others only after 1 day. With this, cannon pod replacement takes a\n",
      "full day per pod.\n",
      "^^^ pre ^^^ # overrides for wire-server/values.yaml\n",
      "cannon:\n",
      "  drainOpts:\n",
      "    # The following defaults drain a minimum of 400 connections/second\n",
      "    # for a total of 10000 over 25 seconds\n",
      "    # (if cannon holds more connections, draining will happen at a faster pace)\n",
      "    gracePeriodSeconds: 25\n",
      "    millisecondsBetweenBatches: 50\n",
      "    minBatchSize: 20\n",
      "\n",
      "$$$ h2 $$$ Control nginz upstreams (routes) into the Kubernetes cluster\n",
      "^^^ p ^^^ Open unterminated upstreams (routes) into the Kubernetes cluster are a potential\n",
      "security issue. To prevent this, there are fine-grained settings in the nginz\n",
      "configuration defining which upstreams should exist.\n",
      "$$$ h3 $$$ Default upstreams\n",
      "^^^ p ^^^ Upstreams for services that exist in (almost) every Wire installation are\n",
      "enabled by default. These are:\n",
      "^^^ p ^^^ brig\n",
      "^^^ p ^^^ cannon\n",
      "^^^ p ^^^ cargohold\n",
      "^^^ p ^^^ galley\n",
      "^^^ p ^^^ gundeck\n",
      "^^^ p ^^^ spar\n",
      "^^^ p ^^^ For special setups (as e.g. described in [separate-websocket-traffic]) the\n",
      "upstreams of these services can be ignored (disabled) with the setting\n",
      "nginz.nginx_conf.ignored_upstreams.\n",
      "^^^ p ^^^ The most common example is to disable the upstream of cannon:\n",
      "^^^ pre ^^^ nginz:\n",
      "  nginx_conf:\n",
      "    ignored_upstreams: [\"cannon\"]\n",
      "\n",
      "$$$ h3 $$$ Optional upstreams\n",
      "^^^ p ^^^ There are some services that are usually not deployed on most Wire installations\n",
      "or are specific to the Wire cloud:\n",
      "^^^ p ^^^ ibis\n",
      "^^^ p ^^^ galeb\n",
      "^^^ p ^^^ calling-test\n",
      "^^^ p ^^^ proxy\n",
      "^^^ p ^^^ The upstreams for those are disabled by default and can be enabled by the\n",
      "setting nginz.nginx_conf.enabled_extra_upstreams.\n",
      "^^^ p ^^^ The most common example is to enable the (extra) upstream of proxy:\n",
      "^^^ pre ^^^ nginz:\n",
      "  nginx_conf:\n",
      "    enabled_extra_upstreams: [\"proxy\"]\n",
      "\n",
      "$$$ h3 $$$ Combining default and extra upstream configurations\n",
      "^^^ p ^^^ Default and extra upstream configurations are independent of each other. I.e.\n",
      "nginz.nginx_conf.ignored_upstreams and\n",
      "nginz.nginx_conf.enabled_extra_upstreams can be combined in the same\n",
      "configuration:\n",
      "^^^ pre ^^^ nginz:\n",
      "  nginx_conf:\n",
      "    ignored_upstreams: [\"cannon\"]\n",
      "    enabled_extra_upstreams: [\"proxy\"]\n",
      "\n",
      "$$$ h2 $$$ Separate incoming websocket network traffic from the rest of the https traffic\n",
      "^^^ p ^^^ By default, incoming network traffic for websockets comes through these network\n",
      "hops:\n",
      "^^^ p ^^^ Internet -> LoadBalancer -> kube-proxy -> nginx-ingress-controller -> nginz -> cannon\n",
      "^^^ p ^^^ In order to have graceful draining of websockets when something gets restarted, as it is not easily\n",
      "possible to implement the graceful draining on nginx-ingress-controller or nginz by itself, there is\n",
      "a configuration option to get the following network hops:\n",
      "^^^ p ^^^ Internet -> separate LoadBalancer for cannon only -> kube-proxy -> [nginz->cannon (2 containers in the same pod)]\n",
      "^^^ pre ^^^ # example on AWS when using cert-manager for TLS certificates and external-dns for DNS records\n",
      "# (see wire-server/charts/cannon/values.yaml for more possible options)\n",
      "\n",
      "# in your wire-server/values.yaml overrides:\n",
      "cannon:\n",
      "  service:\n",
      "    nginz:\n",
      "      enabled: true\n",
      "      hostname: \"nginz-ssl.example.com\"\n",
      "      externalDNS:\n",
      "        enabled: true\n",
      "      certManager:\n",
      "        enabled: true\n",
      "      annotations:\n",
      "        service.beta.kubernetes.io/aws-load-balancer-type: \"nlb\"\n",
      "        service.beta.kubernetes.io/aws-load-balancer-scheme: \"internet-facing\"\n",
      "nginz:\n",
      "  nginx_conf:\n",
      "    ignored_upstreams: [\"cannon\"]\n",
      "\n",
      "^^^ pre ^^^ # in your wire-server/secrets.yaml overrides:\n",
      "cannon:\n",
      "  secrets:\n",
      "    nginz:\n",
      "      zAuth:\n",
      "        publicKeys: ... # same values as in nginz.secrets.zAuth.publicKeys\n",
      "\n",
      "^^^ pre ^^^ # in your nginx-ingress-services/values.yaml overrides:\n",
      "websockets:\n",
      "  enabled: false\n",
      "\n",
      "$$$ h2 $$$ You may want\n",
      "^^^ p ^^^ more server resources to ensure\n",
      "high-availability\n",
      "^^^ p ^^^ an email/SMTP server to send out registration emails\n",
      "^^^ p ^^^ depending on your required functionality, you may or may not need an\n",
      "AWS account. See details about\n",
      "limitations without an AWS account in the following sections.\n",
      "^^^ p ^^^ one or more people able to maintain the installation\n",
      "^^^ p ^^^ official support by Wire (contact us)\n",
      "^^^ p ^^^ Warning\n",
      "^^^ p ^^^ As of 2020-08-10, the documentation sections below are partially out of date and need to be updated.\n",
      "$$$ h2 $$$ Metrics/logging\n",
      "^^^ p ^^^ Monitoring wire-server using Prometheus and Grafana\n",
      "^^^ p ^^^ Installing centralized logging dashboards using Kibana\n",
      "$$$ h2 $$$ SMTP server\n",
      "^^^ p ^^^ Assumptions: none\n",
      "^^^ p ^^^ Provides:\n",
      "^^^ p ^^^ full control over email sending\n",
      "^^^ p ^^^ You need:\n",
      "^^^ p ^^^ SMTP credentials (to allow for email sending; prerequisite for\n",
      "registering users and running the smoketest)\n",
      "^^^ p ^^^ How to configure:\n",
      "^^^ p ^^^ if using a gmail account, ensure to enable ‘less secure\n",
      "apps’\n",
      "^^^ p ^^^ Add user, SMTP server, connection type to values/wire-server’s\n",
      "values file under brig.config.smtp\n",
      "^^^ p ^^^ Add password in secrets/wire-server’s secrets file under\n",
      "brig.secrets.smtpPassword\n",
      "$$$ h2 $$$ Load balancer on bare metal servers\n",
      "^^^ p ^^^ Assumptions:\n",
      "^^^ p ^^^ You installed kubernetes on bare metal servers or virtual machines\n",
      "that can bind to a public IP address.\n",
      "^^^ p ^^^ If you are using AWS or another cloud provider, seeCreating a\n",
      "cloudprovider-based load\n",
      "balancerinstead\n",
      "^^^ p ^^^ Provides:\n",
      "^^^ p ^^^ Allows using a provided Load balancer for incoming traffic\n",
      "^^^ p ^^^ SSL termination is done on the ingress controller\n",
      "^^^ p ^^^ You can access your wire-server backend with given DNS names, over\n",
      "SSL and from anywhere in the internet\n",
      "^^^ p ^^^ You need:\n",
      "^^^ p ^^^ A kubernetes node with a public IP address (or internal, if you do\n",
      "not plan to expose the Wire backend over the Internet but we will\n",
      "assume you are using a public IP address)\n",
      "^^^ p ^^^ DNS records for the different exposed addresses (the ingress depends\n",
      "on the usage of virtual hosts), namely:\n",
      "^^^ p ^^^ nginz-https.<domain>\n",
      "^^^ p ^^^ nginz-ssl.<domain>\n",
      "^^^ p ^^^ assets.<domain>\n",
      "^^^ p ^^^ webapp.<domain>\n",
      "^^^ p ^^^ account.<domain>\n",
      "^^^ p ^^^ teams.<domain>\n",
      "^^^ p ^^^ A wildcard certificate for the different hosts (*.<domain>) - we\n",
      "assume you want to do SSL termination on the ingress controller\n",
      "^^^ p ^^^ Caveats:\n",
      "^^^ p ^^^ Note that there can be only a single load balancer, otherwise your\n",
      "cluster might become\n",
      "unstable\n",
      "^^^ p ^^^ How to configure:\n",
      "^^^ pre ^^^ cp values/metallb/demo-values.example.yaml values/metallb/demo-values.yaml\n",
      "cp values/nginx-ingress-services/demo-values.example.yaml values/nginx-ingress-services/demo-values.yaml\n",
      "cp values/nginx-ingress-services/demo-secrets.example.yaml values/nginx-ingress-services/demo-secrets.yaml\n",
      "\n",
      "^^^ p ^^^ Adapt values/metallb/demo-values.yaml to provide a list of public\n",
      "IP address CIDRs that your kubernetes nodes can bind to.\n",
      "^^^ p ^^^ Adapt values/nginx-ingress-services/demo-values.yaml with correct URLs\n",
      "^^^ p ^^^ Put your TLS cert and key into\n",
      "values/nginx-ingress-services/demo-secrets.yaml.\n",
      "^^^ p ^^^ Install metallb (for more information see the\n",
      "docs):\n",
      "^^^ pre ^^^ helm upgrade --install --namespace metallb-system metallb wire/metallb \\\n",
      "    -f values/metallb/demo-values.yaml \\\n",
      "    --wait --timeout 1800\n",
      "\n",
      "^^^ p ^^^ Install nginx-ingress-[controller,services]:\n",
      "^^^ p ^^^ ::\n",
      ": helm upgrade –install –namespace demo demo-nginx-ingress-controller wire/nginx-ingress-controller\n",
      "^^^ p ^^^ : –wait\n",
      "^^^ p ^^^ helm upgrade –install –namespace demo demo-nginx-ingress-services wire/nginx-ingress-services\n",
      "^^^ p ^^^ : -f values/nginx-ingress-services/demo-values.yaml -f values/nginx-ingress-services/demo-secrets.yaml –wait\n",
      "^^^ p ^^^ Now, create DNS records for the URLs configured above.\n",
      "$$$ h2 $$$ Load Balancer on cloud-provider\n",
      "$$$ h3 $$$ AWS\n",
      "^^^ p ^^^ Upload the required\n",
      "certificates.\n",
      "Create and configure values/aws-ingress/demo-values.yaml from the\n",
      "examples.\n",
      "^^^ pre ^^^ helm upgrade --install --namespace demo demo-aws-ingress wire/aws-ingress \\\n",
      "    -f values/aws-ingress/demo-values.yaml \\\n",
      "    --wait\n",
      "\n",
      "^^^ p ^^^ To give your load balancers public DNS names, create and edit\n",
      "values/external-dns/demo-values.yaml, then run\n",
      "external-dns:\n",
      "^^^ pre ^^^ helm repo update\n",
      "helm upgrade --install --namespace demo demo-external-dns stable/external-dns \\\n",
      "    --version 1.7.3 \\\n",
      "    -f values/external-dns/demo-values.yaml \\\n",
      "    --wait\n",
      "\n",
      "^^^ p ^^^ Things to note about external-dns:\n",
      "^^^ p ^^^ There can only be a single external-dns chart installed (one per\n",
      "kubernetes cluster, not one per namespace). So if you already have\n",
      "one running for another namespace you probably don’t need to do\n",
      "anything.\n",
      "^^^ p ^^^ You have to add the appropriate IAM permissions to your cluster (see\n",
      "the\n",
      "README).\n",
      "^^^ p ^^^ Alternatively, use the AWS route53 console.\n",
      "$$$ h3 $$$ Other cloud providers\n",
      "^^^ p ^^^ This information is not yet available. If you’d like to contribute by\n",
      "adding this information for your cloud provider, feel free to read the\n",
      "contributing guidelines and open a PR.\n",
      "$$$ h2 $$$ Real AWS services\n",
      "^^^ p ^^^ Assumptions:\n",
      "^^^ p ^^^ You installed kubernetes and wire-server on AWS\n",
      "^^^ p ^^^ Provides:\n",
      "^^^ p ^^^ Better availability guarantees and possibly better functionality of\n",
      "AWS services such as SQS and dynamoDB.\n",
      "^^^ p ^^^ You can use ELBs in front of nginz for higher availability.\n",
      "^^^ p ^^^ instead of using a smtp server and connect with SMTP, you may use\n",
      "SES. See configuration of brig and the useSES toggle.\n",
      "^^^ p ^^^ You need:\n",
      "^^^ p ^^^ An AWS account\n",
      "^^^ p ^^^ How to configure:\n",
      "^^^ p ^^^ Instead of using fake-aws charts, you need to set up the respective\n",
      "services in your account, create queues, tables etc. Have a look at\n",
      "the fake-aws-* charts; you’ll need to replicate a similar setup.\n",
      "^^^ p ^^^ Once real AWS resources are created, adapt the configuration in\n",
      "the values and secrets files for wire-server to use real endpoints\n",
      "and real AWS keys. Look for comments including\n",
      "if using real AWS.\n",
      "^^^ p ^^^ Creating AWS resources in a way that is easy to create and delete\n",
      "could be done using either terraform\n",
      "or pulumi. If you’d like to contribute by\n",
      "creating such automation, feel free to read the contributing\n",
      "guidelines and open a PR.\n",
      "$$$ h2 $$$ Persistence and high-availability\n",
      "^^^ p ^^^ Currently, due to the way kubernetes and cassandra\n",
      "interact,\n",
      "cassandra cannot reliably be installed on kubernetes. Some people have\n",
      "tried, e.g. this\n",
      "project though at\n",
      "the time of writing (Nov 2018), this does not yet work as advertised. We\n",
      "recommend therefore to install cassandra, (possibly also elasticsearch\n",
      "and redis) separately, i.e. outside of kubernetes (using 3 nodes each).\n",
      "^^^ p ^^^ For further higher-availability:\n",
      "^^^ p ^^^ scale your kubernetes cluster to have separate etcd and master nodes\n",
      "(3 nodes each)\n",
      "^^^ p ^^^ use 3 instead of 1 replica of each wire-server chart\n",
      "$$$ h2 $$$ Security\n",
      "^^^ p ^^^ For a production deployment, you should, as a minimum:\n",
      "^^^ p ^^^ Ensure traffic between kubernetes nodes, etcd and databases are\n",
      "confined to a private network\n",
      "^^^ p ^^^ Ensure kubernetes API is unreachable from the public internet (e.g.\n",
      "put behind VPN/bastion host or restrict IP range) to prevent\n",
      "kubernetes\n",
      "vulnerabilities\n",
      "from affecting you\n",
      "^^^ p ^^^ Ensure your operating systems get security updates automatically\n",
      "^^^ p ^^^ Restrict ssh access / harden sshd configuration\n",
      "^^^ p ^^^ Ensure no other pods with public access than the main ingress are\n",
      "deployed on your cluster, since, in the current setup, pods have\n",
      "access to etcd values (and thus any secrets stored there, including\n",
      "secrets from other pods)\n",
      "^^^ p ^^^ Ensure developers encrypt any secrets.yaml files\n",
      "^^^ p ^^^ Additionally, you may wish to build, sign, and host your own docker\n",
      "images to have increased confidence in those images. We haved “signed\n",
      "container images” on our roadmap.\n",
      "$$$ h2 $$$ 3rd-party proxying\n",
      "^^^ p ^^^ You need Giphy/Google/Spotify/Soundcloud API keys (if you want to\n",
      "support previews by proxying these services)\n",
      "^^^ p ^^^ See the proxy chart for configuration.\n",
      "$$$ h2 $$$ Routing traffic to other namespaces via nginz\n",
      "^^^ p ^^^ If you have some components running in namespaces different from nginz. For\n",
      "instance, the billing service (ibis) could be deployed to a separate\n",
      "namespace, say integrations. But it still needs to get traffic via\n",
      "nginz. When this is needed, the helm config can be adjusted like this:\n",
      "^^^ pre ^^^ # in your wire-server/values.yaml overrides:\n",
      "nginz:\n",
      "  nginx_conf:\n",
      "    upstream_namespace:\n",
      "      ibis: integrations\n",
      "\n",
      "$$$ h2 $$$ Marking an installation as self-hosted\n",
      "^^^ p ^^^ In case your wire installation is self-hosted (on-premise, demo installs), it needs to be aware that it is through a configuration option.  As of release chart 4.15.0, \"true\" is the default behavior, and nothing needs to be done.\n",
      "^^^ p ^^^ If that option is not set, team-settings will prompt users about “wire for free” and associated functions.\n",
      "^^^ p ^^^ With that option set, all payment related functionality is disabled.\n",
      "^^^ p ^^^ The option is IS_SELF_HOSTED, and you set it in your values.yaml file (originally a copy of prod-values.example.yaml found in wire-server-deploy/values/wire-server/).\n",
      "^^^ p ^^^ In case of a demo install, replace prod with demo.\n",
      "^^^ p ^^^ First set the option under the team-settings section, envVars sub-section:\n",
      "^^^ pre ^^^ envVars:\n",
      "  IS_SELF_HOSTED: \"true\"\n",
      "\n",
      "^^^ p ^^^ Second, also set the option for account-pages helm chart:\n",
      "^^^ pre ^^^ envVars:\n",
      "  IS_SELF_HOSTED: \"true\"\n",
      "\n",
      "$$$ h2 $$$ Configuring authentication cookie throttling\n",
      "^^^ p ^^^ Authentication cookies and the related throttling mechanism is described in the API documentation:\n",
      "Cookies\n",
      "^^^ p ^^^ The maximum number of cookies per account and type is defined by the brig option\n",
      "setUserCookieLimit. Its default is 32.\n",
      "^^^ p ^^^ Throttling is configured by the brig option setUserCookieThrottle. It is an\n",
      "object that contains two fields:\n",
      "^^^ p ^^^ stdDev\n",
      "^^^ p ^^^ : The minimal standard deviation of cookie creation timestamps in\n",
      "Seconds. (Default: 3000,\n",
      "Wikipedia: Standard deviation)\n",
      "^^^ p ^^^ retryAfter\n",
      "^^^ p ^^^ : Wait time in Seconds when stdDev is violated. (Default: 86400)\n",
      "^^^ p ^^^ The default values are fine for most use cases. (Generally, you don’t have to\n",
      "configure them for your installation.)\n",
      "^^^ p ^^^ Condensed example:\n",
      "^^^ pre ^^^ brig:\n",
      "    optSettings:\n",
      "        setUserCookieLimit: 32\n",
      "        setUserCookieThrottle:\n",
      "            stdDev: 3000\n",
      "            retryAfter: 86400\n",
      "\n",
      "$$$ h2 $$$ S3 Addressing Style\n",
      "^^^ p ^^^ S3 can either by addressed in path style, i.e.\n",
      "https://<s3-endpoint>/<bucket-name>/<object>, or vhost style, i.e.\n",
      "https://<bucket-name>.<s3-endpoint>/<object>. AWS’s S3 offering has deprecated\n",
      "path style addressing for S3 and completely disabled it for buckets created\n",
      "after 30 Sep 2020:\n",
      "https://aws.amazon.com/blogs/aws/amazon-s3-path-deprecation-plan-the-rest-of-the-story/\n",
      "^^^ p ^^^ However other object storage providers (specially self-deployed ones like MinIO)\n",
      "may not support vhost style addressing yet (or ever?). Users of such buckets\n",
      "should configure this option to “path”:\n",
      "^^^ pre ^^^ cargohold:\n",
      "  aws:\n",
      "    s3AddressingStyle: path\n",
      "\n",
      "^^^ p ^^^ Installations using S3 service provided by AWS, should use “auto”, this option\n",
      "will ensure that vhost style is only used when it is possible to construct a\n",
      "valid hostname from the bucket name and the bucket name doesn’t contain a ‘.’.\n",
      "Having a ‘.’ in the bucket name causes TLS validation to fail, hence it is not\n",
      "used by default:\n",
      "^^^ pre ^^^ cargohold:\n",
      "  aws:\n",
      "    s3AddressingStyle: auto\n",
      "\n",
      "^^^ p ^^^ Using “virtual” as an option is only useful in situations where vhost style\n",
      "addressing must be used even if it is not possible to construct a valid hostname\n",
      "from the bucket name or the S3 service provider can ensure correct certificate\n",
      "is issued for bucket which contain one or more ‘.’s in the name:\n",
      "^^^ pre ^^^ cargohold:\n",
      "  aws:\n",
      "    s3AddressingStyle: virtual\n",
      "\n",
      "^^^ p ^^^ When this option is unspecified, wire-server defaults to path style addressing\n",
      "to ensure smooth transition for older deployments.\n",
      "$$$ h2 $$$ I have a team larger than 500 users\n",
      "^^^ p ^^^ By default, the maximum number of users in a team is set at 500.\n",
      "^^^ p ^^^ This can be changed in the Brig config, with this option:\n",
      "^^^ pre ^^^     optSettings:\n",
      "      setMaxTeamSize: 501\n",
      "\n",
      "^^^ p ^^^ Note\n",
      "^^^ p ^^^ If you create a team with more than 2000 members then clients won’t receive certain team update events (e.g. new member joining) live via websocket anymore, but most of the app will still function normally.\n",
      "^^^ p ^^^ Irrespective of team size conversations can have at most 2000 members. This limit cannot be overridden.\n",
      "^^^ p ^^^ Wire’s backend currently only supports fanning out a single message to at most 2000 recipients, hence the limitations on conversation size. Increasing this limit is work in progress.\n",
      "^^^ p ^^^ © Copyright 2019 - 2023, Wire Swiss GmbH.\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T15:54:20.337339Z",
     "start_time": "2024-10-01T15:54:20.333040Z"
    }
   },
   "cell_type": "code",
   "source": "print(content_by_headline['Redirect some traffic through a http(s) proxy'])",
   "id": "f6c734540e63c51b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "In case you wish to use http(s) proxies, you can add a configuration like this to the wire-server services in question:\n",
      "Assuming your proxy can be reached from within Kubernetes at http://proxy:8080, add the following for each affected service (e.g. gundeck) to your Helm overrides in values/wire-server/values.yaml :\n",
      "gundeck:\n",
      "  # ...\n",
      "  config:\n",
      "    # ...\n",
      "    proxy:\n",
      "      httpProxy: \"http://proxy:8080\"\n",
      "      httpsProxy: \"http://proxy:8080\"\n",
      "      noProxyList:\n",
      "        - \"localhost\"\n",
      "        - \"127.0.0.1\"\n",
      "        - \"10.0.0.0/8\"\n",
      "        - \"elasticsearch-external\"\n",
      "        - \"cassandra-external\"\n",
      "        - \"redis-ephemeral\"\n",
      "        - \"fake-aws-sqs\"\n",
      "        - \"fake-aws-dynamodb\"\n",
      "        - \"fake-aws-sns\"\n",
      "        - \"brig\"\n",
      "        - \"cargohold\"\n",
      "        - \"galley\"\n",
      "        - \"gundeck\"\n",
      "        - \"proxy\"\n",
      "        - \"spar\"\n",
      "        - \"federator\"\n",
      "        - \"cannon\"\n",
      "        - \"cannon-0.cannon.default\"\n",
      "        - \"cannon-1.cannon.default\"\n",
      "        - \"cannon-2.cannon.default\"\n",
      "\n",
      "Depending on your setup, you may need to repeat this for the other services like brig as well.\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T15:55:47.761695Z",
     "start_time": "2024-10-01T15:55:47.757416Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for key, content in content_by_headline.items():\n",
    "    print(f\"{key}{content}\\n\\n\\n\")"
   ],
   "id": "2ce05f742336fb39",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Infrastructure configuration options\n",
      "This contains instructions to configure specific aspects of your production setup depending on your needs.\n",
      "Depending on your use-case and requirements, you may need to configure none, or only a subset of the following sections.\n",
      "\n",
      "\n",
      "\n",
      "Redirect some traffic through a http(s) proxy\n",
      "In case you wish to use http(s) proxies, you can add a configuration like this to the wire-server services in question:\n",
      "Assuming your proxy can be reached from within Kubernetes at http://proxy:8080, add the following for each affected service (e.g. gundeck) to your Helm overrides in values/wire-server/values.yaml :\n",
      "gundeck:\n",
      "  # ...\n",
      "  config:\n",
      "    # ...\n",
      "    proxy:\n",
      "      httpProxy: \"http://proxy:8080\"\n",
      "      httpsProxy: \"http://proxy:8080\"\n",
      "      noProxyList:\n",
      "        - \"localhost\"\n",
      "        - \"127.0.0.1\"\n",
      "        - \"10.0.0.0/8\"\n",
      "        - \"elasticsearch-external\"\n",
      "        - \"cassandra-external\"\n",
      "        - \"redis-ephemeral\"\n",
      "        - \"fake-aws-sqs\"\n",
      "        - \"fake-aws-dynamodb\"\n",
      "        - \"fake-aws-sns\"\n",
      "        - \"brig\"\n",
      "        - \"cargohold\"\n",
      "        - \"galley\"\n",
      "        - \"gundeck\"\n",
      "        - \"proxy\"\n",
      "        - \"spar\"\n",
      "        - \"federator\"\n",
      "        - \"cannon\"\n",
      "        - \"cannon-0.cannon.default\"\n",
      "        - \"cannon-1.cannon.default\"\n",
      "        - \"cannon-2.cannon.default\"\n",
      "\n",
      "Depending on your setup, you may need to repeat this for the other services like brig as well.\n",
      "\n",
      "\n",
      "\n",
      "Enable push notifications using the public appstore / playstore mobile Wire clients\n",
      "You need to get in touch with us. Please talk to sales or customer support - see https://wire.com\n",
      "If a contract agreement has been reached, we can set up a separate AWS account for you containing the necessary AWS SQS/SNS setup to route push notifications through to the mobile apps. We will then forward some configuration / access credentials that looks like:\n",
      "gundeck:\n",
      "  config:\n",
      "    aws:\n",
      "      account: \"<REDACTED>\"\n",
      "      arnEnv: \"<REDACTED>\"\n",
      "      queueName: \"<REDACTED>-gundeck-events\"\n",
      "      region: \"<REDACTED>\"\n",
      "      snsEndpoint: \"https://sns.<REDACTED>.amazonaws.com\"\n",
      "      sqsEndpoint: \"https://sqs.<REDACTED>.amazonaws.com\"\n",
      "  secrets:\n",
      "    awsKeyId: \"<REDACTED>\"\n",
      "    awsSecretKey: \"<REDACTED>\"\n",
      "\n",
      "To make use of those, first test the credentials are correct, e.g. using the aws command-line tool (for more information on how to configure credentials, please refer to the official docs):\n",
      "AWS_REGION=<region>\n",
      "AWS_ACCESS_KEY_ID=<...>\n",
      "AWS_SECRET_ACCESS_KEY=<...>\n",
      "ENV=<environment> #e.g staging\n",
      "\n",
      "aws sqs get-queue-url --queue-name \"$ENV-gundeck-events\"\n",
      "\n",
      "You should get a result like this:\n",
      "{\n",
      "    \"QueueUrl\": \"https://<region>.queue.amazonaws.com/<aws-account-id>/<environment>-gundeck-events\"\n",
      "}\n",
      "\n",
      "Then add them to your gundeck configuration overrides.\n",
      "Keys below gundeck.config belong into values/wire-server/values.yaml:\n",
      "gundeck:\n",
      "  # ...\n",
      "  config:\n",
      "    aws:\n",
      "      queueName: # e.g. staging-gundeck-events\n",
      "      account: # <aws-account-id>, e.g. 123456789\n",
      "      region: # e.g. eu-central-1\n",
      "      snsEndpoint: # e.g. https://sns.eu-central-1.amazonaws.com\n",
      "      sqsEndpoint: # e.g. https://sqs.eu-central-1.amazonaws.com\n",
      "      arnEnv: # e.g. staging - this must match the environment name (first part of queueName)\n",
      "\n",
      "Keys below gundeck.secrets belong into values/wire-server/secrets.yaml:\n",
      "gundeck:\n",
      "  # ...\n",
      "  secrets:\n",
      "    awsKeyId: CHANGE-ME\n",
      "    awsSecretKey: CHANGE-ME\n",
      "\n",
      "After making this change and applying it to gundeck (ensure gundeck pods have restarted to make use of the updated configuration - that should happen automatically), make sure to reset the push token on any mobile devices that you may have in use.\n",
      "Next, if you want, you can stop using the fake-aws-sns pods in case you ran them before:\n",
      "# inside override values/fake-aws/values.yaml\n",
      "fake-aws-sns:\n",
      "  enabled: false\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Controlling the speed of websocket draining during cannon pod replacement\n",
      "The ‘cannon’ component is responsible for persistent websocket connections. Normally the default options would slowly and gracefully drain active websocket connections over a maximum of (amount of cannon replicas * 30 seconds) during the deployment of a new wire-server version. This will lead to a very brief interruption for Wire clients when their client has to re-connect on the websocket.\n",
      "You’re not expected to need to change these settings.\n",
      "The following options are only relevant during the restart of cannon itself. During a restart of nginz or ingress-controller, all websockets will get severed. If this is to be avoided, see section Separate incoming websocket network traffic from the rest of the https traffic\n",
      "drainOpts: Drain websockets in a controlled fashion when cannon receives a SIGTERM or SIGINT (this happens when a pod is terminated e.g. during rollout of a new version). Instead of waiting for connections to close on their own, the websockets are now severed at a controlled pace. This allows for quicker rollouts of new versions.\n",
      "There is no way to entirely disable this behaviour, two extreme examples below\n",
      "the quickest way to kill cannon is to set gracePeriodSeconds: 1 and minBatchSize: 100000 which would sever all connections immediately; but it’s not recommended as you could DDoS yourself by forcing all active clients to reconnect at the same time. With this, cannon pod replacement takes only 1 second per pod.\n",
      "the slowest way to roll out a new version of cannon without severing websocket connections for a long time is to set minBatchSize: 1, millisecondsBetweenBatches: 86400000 and gracePeriodSeconds: 86400 which would lead to one single websocket connection being closed immediately, and all others only after 1 day. With this, cannon pod replacement takes a full day per pod.\n",
      "# overrides for wire-server/values.yaml\n",
      "cannon:\n",
      "  drainOpts:\n",
      "    # The following defaults drain a minimum of 400 connections/second\n",
      "    # for a total of 10000 over 25 seconds\n",
      "    # (if cannon holds more connections, draining will happen at a faster pace)\n",
      "    gracePeriodSeconds: 25\n",
      "    millisecondsBetweenBatches: 50\n",
      "    minBatchSize: 20\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Control nginz upstreams (routes) into the Kubernetes cluster\n",
      "Open unterminated upstreams (routes) into the Kubernetes cluster are a potential security issue. To prevent this, there are fine-grained settings in the nginz configuration defining which upstreams should exist.\n",
      "\n",
      "\n",
      "\n",
      "Default upstreams\n",
      "Upstreams for services that exist in (almost) every Wire installation are enabled by default. These are:\n",
      "brig\n",
      "cannon\n",
      "cargohold\n",
      "galley\n",
      "gundeck\n",
      "spar\n",
      "For special setups (as e.g. described in [separate-websocket-traffic]) the upstreams of these services can be ignored (disabled) with the setting nginz.nginx_conf.ignored_upstreams.\n",
      "The most common example is to disable the upstream of cannon:\n",
      "nginz:\n",
      "  nginx_conf:\n",
      "    ignored_upstreams: [\"cannon\"]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Optional upstreams\n",
      "There are some services that are usually not deployed on most Wire installations or are specific to the Wire cloud:\n",
      "ibis\n",
      "galeb\n",
      "calling-test\n",
      "proxy\n",
      "The upstreams for those are disabled by default and can be enabled by the setting nginz.nginx_conf.enabled_extra_upstreams.\n",
      "The most common example is to enable the (extra) upstream of proxy:\n",
      "nginz:\n",
      "  nginx_conf:\n",
      "    enabled_extra_upstreams: [\"proxy\"]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Combining default and extra upstream configurations\n",
      "Default and extra upstream configurations are independent of each other. I.e. nginz.nginx_conf.ignored_upstreams and nginz.nginx_conf.enabled_extra_upstreams can be combined in the same configuration:\n",
      "nginz:\n",
      "  nginx_conf:\n",
      "    ignored_upstreams: [\"cannon\"]\n",
      "    enabled_extra_upstreams: [\"proxy\"]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Separate incoming websocket network traffic from the rest of the https traffic\n",
      "By default, incoming network traffic for websockets comes through these network hops:\n",
      "Internet -> LoadBalancer -> kube-proxy -> nginx-ingress-controller -> nginz -> cannon\n",
      "In order to have graceful draining of websockets when something gets restarted, as it is not easily possible to implement the graceful draining on nginx-ingress-controller or nginz by itself, there is a configuration option to get the following network hops:\n",
      "Internet -> separate LoadBalancer for cannon only -> kube-proxy -> [nginz->cannon (2 containers in the same pod)]\n",
      "# example on AWS when using cert-manager for TLS certificates and external-dns for DNS records\n",
      "# (see wire-server/charts/cannon/values.yaml for more possible options)\n",
      "\n",
      "# in your wire-server/values.yaml overrides:\n",
      "cannon:\n",
      "  service:\n",
      "    nginz:\n",
      "      enabled: true\n",
      "      hostname: \"nginz-ssl.example.com\"\n",
      "      externalDNS:\n",
      "        enabled: true\n",
      "      certManager:\n",
      "        enabled: true\n",
      "      annotations:\n",
      "        service.beta.kubernetes.io/aws-load-balancer-type: \"nlb\"\n",
      "        service.beta.kubernetes.io/aws-load-balancer-scheme: \"internet-facing\"\n",
      "nginz:\n",
      "  nginx_conf:\n",
      "    ignored_upstreams: [\"cannon\"]\n",
      "\n",
      "# in your wire-server/secrets.yaml overrides:\n",
      "cannon:\n",
      "  secrets:\n",
      "    nginz:\n",
      "      zAuth:\n",
      "        publicKeys: ... # same values as in nginz.secrets.zAuth.publicKeys\n",
      "\n",
      "# in your nginx-ingress-services/values.yaml overrides:\n",
      "websockets:\n",
      "  enabled: false\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "You may want\n",
      "more server resources to ensure high-availability\n",
      "an email/SMTP server to send out registration emails\n",
      "depending on your required functionality, you may or may not need an AWS account. See details about limitations without an AWS account in the following sections.\n",
      "one or more people able to maintain the installation\n",
      "official support by Wire (contact us)\n",
      "Warning\n",
      "As of 2020-08-10, the documentation sections below are partially out of date and need to be updated.\n",
      "\n",
      "\n",
      "\n",
      "Metrics/logging\n",
      "Monitoring wire-server using Prometheus and Grafana\n",
      "Installing centralized logging dashboards using Kibana\n",
      "\n",
      "\n",
      "\n",
      "SMTP server\n",
      "Assumptions: none\n",
      "Provides:\n",
      "full control over email sending\n",
      "You need:\n",
      "SMTP credentials (to allow for email sending; prerequisite for registering users and running the smoketest)\n",
      "How to configure:\n",
      "if using a gmail account, ensure to enable ‘less secure apps’\n",
      "Add user, SMTP server, connection type to values/wire-server’s values file under brig.config.smtp\n",
      "Add password in secrets/wire-server’s secrets file under brig.secrets.smtpPassword\n",
      "\n",
      "\n",
      "\n",
      "Load balancer on bare metal servers\n",
      "Assumptions:\n",
      "You installed kubernetes on bare metal servers or virtual machines that can bind to a public IP address.\n",
      "If you are using AWS or another cloud provider, seeCreating a cloudprovider-based load balancerinstead\n",
      "Provides:\n",
      "Allows using a provided Load balancer for incoming traffic\n",
      "SSL termination is done on the ingress controller\n",
      "You can access your wire-server backend with given DNS names, over SSL and from anywhere in the internet\n",
      "You need:\n",
      "A kubernetes node with a public IP address (or internal, if you do not plan to expose the Wire backend over the Internet but we will assume you are using a public IP address)\n",
      "DNS records for the different exposed addresses (the ingress depends on the usage of virtual hosts), namely:\n",
      "nginz-https.<domain>\n",
      "nginz-ssl.<domain>\n",
      "assets.<domain>\n",
      "webapp.<domain>\n",
      "account.<domain>\n",
      "teams.<domain>\n",
      "A wildcard certificate for the different hosts (*.<domain>) - we assume you want to do SSL termination on the ingress controller\n",
      "Caveats:\n",
      "Note that there can be only a single load balancer, otherwise your cluster might become unstable\n",
      "How to configure:\n",
      "cp values/metallb/demo-values.example.yaml values/metallb/demo-values.yaml\n",
      "cp values/nginx-ingress-services/demo-values.example.yaml values/nginx-ingress-services/demo-values.yaml\n",
      "cp values/nginx-ingress-services/demo-secrets.example.yaml values/nginx-ingress-services/demo-secrets.yaml\n",
      "\n",
      "Adapt values/metallb/demo-values.yaml to provide a list of public IP address CIDRs that your kubernetes nodes can bind to.\n",
      "Adapt values/nginx-ingress-services/demo-values.yaml with correct URLs\n",
      "Put your TLS cert and key into values/nginx-ingress-services/demo-secrets.yaml.\n",
      "Install metallb (for more information see the docs):\n",
      "helm upgrade --install --namespace metallb-system metallb wire/metallb \\\n",
      "    -f values/metallb/demo-values.yaml \\\n",
      "    --wait --timeout 1800\n",
      "\n",
      "Install nginx-ingress-[controller,services]:\n",
      ":: : helm upgrade –install –namespace demo demo-nginx-ingress-controller wire/nginx-ingress-controller\n",
      ": –wait\n",
      "helm upgrade –install –namespace demo demo-nginx-ingress-services wire/nginx-ingress-services\n",
      ": -f values/nginx-ingress-services/demo-values.yaml -f values/nginx-ingress-services/demo-secrets.yaml –wait\n",
      "Now, create DNS records for the URLs configured above.\n",
      "\n",
      "\n",
      "\n",
      "AWS\n",
      "Upload the required certificates. Create and configure values/aws-ingress/demo-values.yaml from the examples.\n",
      "helm upgrade --install --namespace demo demo-aws-ingress wire/aws-ingress \\\n",
      "    -f values/aws-ingress/demo-values.yaml \\\n",
      "    --wait\n",
      "\n",
      "To give your load balancers public DNS names, create and edit values/external-dns/demo-values.yaml, then run external-dns:\n",
      "helm repo update\n",
      "helm upgrade --install --namespace demo demo-external-dns stable/external-dns \\\n",
      "    --version 1.7.3 \\\n",
      "    -f values/external-dns/demo-values.yaml \\\n",
      "    --wait\n",
      "\n",
      "Things to note about external-dns:\n",
      "There can only be a single external-dns chart installed (one per kubernetes cluster, not one per namespace). So if you already have one running for another namespace you probably don’t need to do anything.\n",
      "You have to add the appropriate IAM permissions to your cluster (see the README).\n",
      "Alternatively, use the AWS route53 console.\n",
      "\n",
      "\n",
      "\n",
      "Other cloud providers\n",
      "This information is not yet available. If you’d like to contribute by adding this information for your cloud provider, feel free to read the contributing guidelines and open a PR.\n",
      "\n",
      "\n",
      "\n",
      "Real AWS services\n",
      "Assumptions:\n",
      "You installed kubernetes and wire-server on AWS\n",
      "Provides:\n",
      "Better availability guarantees and possibly better functionality of AWS services such as SQS and dynamoDB.\n",
      "You can use ELBs in front of nginz for higher availability.\n",
      "instead of using a smtp server and connect with SMTP, you may use SES. See configuration of brig and the useSES toggle.\n",
      "You need:\n",
      "An AWS account\n",
      "How to configure:\n",
      "Instead of using fake-aws charts, you need to set up the respective services in your account, create queues, tables etc. Have a look at the fake-aws-* charts; you’ll need to replicate a similar setup.\n",
      "Once real AWS resources are created, adapt the configuration in the values and secrets files for wire-server to use real endpoints and real AWS keys. Look for comments including if using real AWS.\n",
      "Creating AWS resources in a way that is easy to create and delete could be done using either terraform or pulumi. If you’d like to contribute by creating such automation, feel free to read the contributing guidelines and open a PR.\n",
      "\n",
      "\n",
      "\n",
      "Persistence and high-availability\n",
      "Currently, due to the way kubernetes and cassandra interact, cassandra cannot reliably be installed on kubernetes. Some people have tried, e.g. this project though at the time of writing (Nov 2018), this does not yet work as advertised. We recommend therefore to install cassandra, (possibly also elasticsearch and redis) separately, i.e. outside of kubernetes (using 3 nodes each).\n",
      "For further higher-availability:\n",
      "scale your kubernetes cluster to have separate etcd and master nodes (3 nodes each)\n",
      "use 3 instead of 1 replica of each wire-server chart\n",
      "\n",
      "\n",
      "\n",
      "Security\n",
      "For a production deployment, you should, as a minimum:\n",
      "Ensure traffic between kubernetes nodes, etcd and databases are confined to a private network\n",
      "Ensure kubernetes API is unreachable from the public internet (e.g. put behind VPN/bastion host or restrict IP range) to prevent kubernetes vulnerabilities from affecting you\n",
      "Ensure your operating systems get security updates automatically\n",
      "Restrict ssh access / harden sshd configuration\n",
      "Ensure no other pods with public access than the main ingress are deployed on your cluster, since, in the current setup, pods have access to etcd values (and thus any secrets stored there, including secrets from other pods)\n",
      "Ensure developers encrypt any secrets.yaml files\n",
      "Additionally, you may wish to build, sign, and host your own docker images to have increased confidence in those images. We haved “signed container images” on our roadmap.\n",
      "\n",
      "\n",
      "\n",
      "3rd-party proxying\n",
      "You need Giphy/Google/Spotify/Soundcloud API keys (if you want to support previews by proxying these services)\n",
      "See the proxy chart for configuration.\n",
      "\n",
      "\n",
      "\n",
      "Routing traffic to other namespaces via nginz\n",
      "If you have some components running in namespaces different from nginz. For instance, the billing service (ibis) could be deployed to a separate namespace, say integrations. But it still needs to get traffic via nginz. When this is needed, the helm config can be adjusted like this:\n",
      "# in your wire-server/values.yaml overrides:\n",
      "nginz:\n",
      "  nginx_conf:\n",
      "    upstream_namespace:\n",
      "      ibis: integrations\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Marking an installation as self-hosted\n",
      "In case your wire installation is self-hosted (on-premise, demo installs), it needs to be aware that it is through a configuration option. As of release chart 4.15.0, \"true\" is the default behavior, and nothing needs to be done.\n",
      "If that option is not set, team-settings will prompt users about “wire for free” and associated functions.\n",
      "With that option set, all payment related functionality is disabled.\n",
      "The option is IS_SELF_HOSTED, and you set it in your values.yaml file (originally a copy of prod-values.example.yaml found in wire-server-deploy/values/wire-server/).\n",
      "In case of a demo install, replace prod with demo.\n",
      "First set the option under the team-settings section, envVars sub-section:\n",
      "envVars:\n",
      "  IS_SELF_HOSTED: \"true\"\n",
      "\n",
      "Second, also set the option for account-pages helm chart:\n",
      "envVars:\n",
      "  IS_SELF_HOSTED: \"true\"\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Configuring authentication cookie throttling\n",
      "Authentication cookies and the related throttling mechanism is described in the API documentation: Cookies\n",
      "The maximum number of cookies per account and type is defined by the brig option setUserCookieLimit. Its default is 32.\n",
      "Throttling is configured by the brig option setUserCookieThrottle. It is an object that contains two fields:\n",
      "stdDev\n",
      ": The minimal standard deviation of cookie creation timestamps in Seconds. (Default: 3000, Wikipedia: Standard deviation)\n",
      "retryAfter\n",
      ": Wait time in Seconds when stdDev is violated. (Default: 86400)\n",
      "The default values are fine for most use cases. (Generally, you don’t have to configure them for your installation.)\n",
      "Condensed example:\n",
      "brig:\n",
      "    optSettings:\n",
      "        setUserCookieLimit: 32\n",
      "        setUserCookieThrottle:\n",
      "            stdDev: 3000\n",
      "            retryAfter: 86400\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "S3 Addressing Style\n",
      "S3 can either by addressed in path style, i.e. https://<s3-endpoint>/<bucket-name>/<object>, or vhost style, i.e. https://<bucket-name>.<s3-endpoint>/<object>. AWS’s S3 offering has deprecated path style addressing for S3 and completely disabled it for buckets created after 30 Sep 2020: https://aws.amazon.com/blogs/aws/amazon-s3-path-deprecation-plan-the-rest-of-the-story/\n",
      "However other object storage providers (specially self-deployed ones like MinIO) may not support vhost style addressing yet (or ever?). Users of such buckets should configure this option to “path”:\n",
      "cargohold:\n",
      "  aws:\n",
      "    s3AddressingStyle: path\n",
      "\n",
      "Installations using S3 service provided by AWS, should use “auto”, this option will ensure that vhost style is only used when it is possible to construct a valid hostname from the bucket name and the bucket name doesn’t contain a ‘.’. Having a ‘.’ in the bucket name causes TLS validation to fail, hence it is not used by default:\n",
      "cargohold:\n",
      "  aws:\n",
      "    s3AddressingStyle: auto\n",
      "\n",
      "Using “virtual” as an option is only useful in situations where vhost style addressing must be used even if it is not possible to construct a valid hostname from the bucket name or the S3 service provider can ensure correct certificate is issued for bucket which contain one or more ‘.’s in the name:\n",
      "cargohold:\n",
      "  aws:\n",
      "    s3AddressingStyle: virtual\n",
      "\n",
      "When this option is unspecified, wire-server defaults to path style addressing to ensure smooth transition for older deployments.\n",
      "\n",
      "\n",
      "\n",
      "I have a team larger than 500 users\n",
      "By default, the maximum number of users in a team is set at 500.\n",
      "This can be changed in the Brig config, with this option:\n",
      "    optSettings:\n",
      "      setMaxTeamSize: 501\n",
      "\n",
      "Note\n",
      "If you create a team with more than 2000 members then clients won’t receive certain team update events (e.g. new member joining) live via websocket anymore, but most of the app will still function normally.\n",
      "Irrespective of team size conversations can have at most 2000 members. This limit cannot be overridden.\n",
      "Wire’s backend currently only supports fanning out a single message to at most 2000 recipients, hence the limitations on conversation size. Increasing this limit is work in progress.\n",
      "© Copyright 2019 - 2023, Wire Swiss GmbH.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "execution_count": 38
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
