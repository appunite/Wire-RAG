{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/appunite/Wire-RAG/blob/main/main_rag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac9e9f500c62e007",
      "metadata": {
        "collapsed": false,
        "id": "ac9e9f500c62e007"
      },
      "source": [
        "## Wire RAG"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import getpass\n",
        "os.environ[\"PINECONE_API_KEY\"] = getpass.getpass(\"pinecone api key\")\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"open ai api key\")"
      ],
      "metadata": {
        "id": "hgvsByZlcsN7"
      },
      "id": "hgvsByZlcsN7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "L5aAdbvCNIME",
      "metadata": {
        "id": "L5aAdbvCNIME"
      },
      "outputs": [],
      "source": [
        "!pip install haystack-ai pinecone-haystack sentence-transformers pinecone transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b36f5fc7",
      "metadata": {
        "id": "b36f5fc7"
      },
      "source": [
        "### Scrape Urls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "393ea1e7",
      "metadata": {
        "id": "393ea1e7"
      },
      "outputs": [],
      "source": [
        "import aiohttp\n",
        "import asyncio\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin, urlparse\n",
        "import nest_asyncio\n",
        "\n",
        "# Apply the nest_asyncio patch to allow nested event loops in Jupyter\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Asynchronous URL fetching with retry logic\n",
        "async def fetch_urls(url, session, retries=3):\n",
        "    attempt = 0\n",
        "    while attempt < retries:\n",
        "        try:\n",
        "            async with session.get(url, timeout=15) as response:\n",
        "                if response.status != 200:\n",
        "                    return set()  # Return an empty set if the page doesn't load\n",
        "                content = await response.text()\n",
        "                soup = BeautifulSoup(content, \"html.parser\")\n",
        "                urls = set(\n",
        "                    urljoin(url, link['href'])\n",
        "                    for link in soup.find_all('a', href=True)\n",
        "                    if urlparse(urljoin(url, link['href'])).scheme in ('http', 'https')\n",
        "                )\n",
        "                # if attempt > 0:\n",
        "                #     print(f\"Successfully fetched URL: {url} on attempt {attempt + 1}\")\n",
        "                return urls\n",
        "        except (aiohttp.ClientError, asyncio.TimeoutError) as e:\n",
        "            attempt += 1\n",
        "            if attempt < retries:\n",
        "                await asyncio.sleep(2 ** attempt)  # Exponential backoff\n",
        "        except Exception as e:\n",
        "            return set()\n",
        "\n",
        "    print(f\"Failed to fetch URL {url} after {retries} attempts.\")\n",
        "    return set()\n",
        "\n",
        "# Check if URL should be allowed based on blacklist/whitelist mode\n",
        "def is_url_allowed(url, black_list_data):\n",
        "    url_list = black_list_data[\"list\"]\n",
        "    is_blacklist_mode = black_list_data[\"black_list\"]\n",
        "\n",
        "    parsed_url = urlparse(url)\n",
        "    base_url = f\"{parsed_url.scheme}://{parsed_url.netloc}\"\n",
        "\n",
        "    if is_blacklist_mode:\n",
        "        # Blacklist mode: block URLs matching any blacklist entry\n",
        "        for entry in url_list:\n",
        "            entry_parsed = urlparse(entry)\n",
        "            entry_base = f\"{entry_parsed.scheme}://{entry_parsed.netloc}\"\n",
        "            if entry_base == base_url and url.startswith(entry):\n",
        "                return False\n",
        "            # Specifically check if the entry is a path that should be blocked\n",
        "            if entry_parsed.netloc == parsed_url.netloc and urlparse(entry).path == parsed_url.path:\n",
        "                return False\n",
        "    else:\n",
        "        # Whitelist mode: only allow URLs matching any whitelist entry\n",
        "        return any(\n",
        "            url.startswith(entry) or f\"{urlparse(entry).scheme}://{urlparse(entry).netloc}\" == base_url\n",
        "            for entry in url_list\n",
        "        )\n",
        "\n",
        "    return True\n",
        "\n",
        "# Asynchronous scraping with blacklist/whitelist and depth handling\n",
        "async def scrape_urls(url, session, max_depth, current_depth=0, visited=None, black_list_data=None):\n",
        "    if visited is None:\n",
        "        visited = []\n",
        "\n",
        "    if current_depth > max_depth:\n",
        "        return visited  # Stop recursion if depth limit exceeded\n",
        "\n",
        "    if not is_url_allowed(url, black_list_data):\n",
        "        return visited  # Skip URL if not allowed\n",
        "\n",
        "    visited.append(url)  # Store URL with its depth\n",
        "\n",
        "    # Extract URLs from the current page\n",
        "    urls = await fetch_urls(url, session)\n",
        "\n",
        "    tasks = []\n",
        "    for new_url in urls:\n",
        "        if new_url not in visited and is_url_allowed(new_url, black_list_data):\n",
        "            # Continue scraping at the next depth level if within depth limit\n",
        "            if current_depth < max_depth:\n",
        "                tasks.append(scrape_urls(new_url, session, max_depth, current_depth + 1, visited, black_list_data))\n",
        "\n",
        "    # Await all the tasks concurrently\n",
        "    await asyncio.gather(*tasks)\n",
        "    return visited\n",
        "\n",
        "# Entry point for asynchronous scraping\n",
        "async def start_scraping(url, depth, black_list_data=None):\n",
        "    async with aiohttp.ClientSession() as session:\n",
        "        found_urls = await scrape_urls(url, session, depth, black_list_data=black_list_data)\n",
        "    return found_urls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c20d222d",
      "metadata": {
        "id": "c20d222d"
      },
      "outputs": [],
      "source": [
        "starting_url = \"https://docs.wire.com\"\n",
        "depth_limit = 2\n",
        "whitelist = {\"list\": [\"https://docs.wire.com\"], \"black_list\": False}\n",
        "\n",
        "scraped_urls = await start_scraping(starting_url, depth_limit, whitelist)\n",
        "\n",
        "print(f\"Total URLs found: {len(scraped_urls)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5a20874",
      "metadata": {
        "id": "b5a20874"
      },
      "source": [
        "Extract headlines and content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "8005a467",
      "metadata": {
        "id": "8005a467"
      },
      "outputs": [],
      "source": [
        "from genericpath import exists\n",
        "import requests\n",
        "import re\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "from requests.adapters import HTTPAdapter\n",
        "from urllib3.util.retry import Retry\n",
        "from collections import defaultdict\n",
        "\n",
        "def requests_retry_session(\n",
        "    retries=3,\n",
        "    backoff_factor=0.3,\n",
        "    status_forcelist=(500, 502, 504),\n",
        "    session=None,\n",
        "):\n",
        "    session = session or requests.Session()\n",
        "    retry = Retry(\n",
        "        total=retries,\n",
        "        read=retries,\n",
        "        connect=retries,\n",
        "        backoff_factor=backoff_factor,\n",
        "        status_forcelist=status_forcelist,\n",
        "    )\n",
        "    adapter = HTTPAdapter(max_retries=retry)\n",
        "    session.mount('http://', adapter)\n",
        "    session.mount('https://', adapter)\n",
        "    return session\n",
        "\n",
        "\n",
        "def extract_headlines_and_content(url):\n",
        "    try:\n",
        "        # Use retry session for robust requests\n",
        "        session = requests_retry_session()\n",
        "        response = session.get(url, timeout=10)\n",
        "\n",
        "        # Check if request was successful\n",
        "        response.raise_for_status()\n",
        "\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        content_by_headline = {}\n",
        "        current_header = None\n",
        "\n",
        "        # Loop through the elements, keeping track of headlines and paragraphs\n",
        "        for element in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p', 'ul', 'ol']):\n",
        "            if element.name.startswith('h'):\n",
        "                # Headline\n",
        "                current_header = element.get_text(strip=True)\n",
        "                current_header = current_header.replace(\"\\n\", \"\")\n",
        "                current_header = re.sub(r\"\\s+\", \" \", current_header)\n",
        "\n",
        "            elif element.name in ['p', 'ul', 'ol'] and current_header:\n",
        "                # Append the text under the last seen headline\n",
        "                value = content_by_headline[current_header] if current_header in content_by_headline else \"\"\n",
        "                content_by_headline[current_header] = f\"{value} {element.get_text(strip=True)}\"\n",
        "\n",
        "        return content_by_headline\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error fetching {url}: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "5bbccafc",
      "metadata": {
        "id": "5bbccafc"
      },
      "outputs": [],
      "source": [
        "all_documents = []\n",
        "\n",
        "for url in scraped_urls:\n",
        "    content_by_headline = extract_headlines_and_content(url)\n",
        "    # print(content_by_headline, \"\\n\\n\\n\")\n",
        "\n",
        "    all_documents.append(content_by_headline)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f88dbfb9166fe5db",
      "metadata": {
        "collapsed": false,
        "id": "f88dbfb9166fe5db"
      },
      "source": [
        "To delete all records u need to `pip install \"pinecone[grpc]\"` and run the following code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "5142cfb8161abc1",
      "metadata": {
        "id": "5142cfb8161abc1"
      },
      "outputs": [],
      "source": [
        "from haystack.components.preprocessors import DocumentSplitter, DocumentCleaner\n",
        "from haystack.components.embedders import SentenceTransformersDocumentEmbedder\n",
        "from haystack.components.joiners import DocumentJoiner\n",
        "from haystack.components.writers import DocumentWriter\n",
        "from haystack import Pipeline\n",
        "from haystack import Document\n",
        "from haystack_integrations.document_stores.pinecone import PineconeDocumentStore\n",
        "\n",
        "document_store = PineconeDocumentStore(\n",
        "\t\tindex=\"default\",\n",
        "\t\tnamespace=\"default\",\n",
        "\t\tdimension=384,\n",
        "  \tmetric=\"cosine\",\n",
        "  \tspec={\"serverless\": {\"region\": \"us-east-1\", \"cloud\": \"aws\"}}\n",
        ")\n",
        "\n",
        "ready_list = []\n",
        "for d in all_documents:\n",
        "\tfor key in d:\n",
        "\t\theadline = key\n",
        "\t\tcontent = d[key]\n",
        "\tdocument = Document(content=content, meta={\"headline\": headline})\n",
        "\tready_list.append(document)\n",
        "\n",
        "document_joiner = DocumentJoiner()\n",
        "document_cleaner = DocumentCleaner()\n",
        "document_splitter = DocumentSplitter(split_by=\"word\", split_length=150, split_overlap=50)\n",
        "document_embedder = SentenceTransformersDocumentEmbedder(model=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "document_writer = DocumentWriter(document_store)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3bfe34af87648a2a",
      "metadata": {
        "collapsed": false,
        "id": "3bfe34af87648a2a"
      },
      "source": [
        "Create a pipeline to populate the Pinecone Document Store with test case documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0df56147ec0fad8",
      "metadata": {
        "id": "f0df56147ec0fad8"
      },
      "outputs": [],
      "source": [
        "preprocessing_pipeline = Pipeline()\n",
        "\n",
        "preprocessing_pipeline.add_component(instance=document_cleaner, name=\"document_cleaner\")\n",
        "preprocessing_pipeline.add_component(instance=document_splitter, name=\"document_splitter\")\n",
        "preprocessing_pipeline.add_component(instance=document_embedder, name=\"document_embedder\")\n",
        "preprocessing_pipeline.add_component(instance=document_writer, name=\"document_writer\")\n",
        "preprocessing_pipeline.add_component(instance=document_joiner, name=\"document_joiner\")\n",
        "\n",
        "preprocessing_pipeline.connect(\"document_joiner\", \"document_cleaner\")\n",
        "preprocessing_pipeline.connect(\"document_cleaner\", \"document_splitter\")\n",
        "preprocessing_pipeline.connect(\"document_splitter\", \"document_embedder\")\n",
        "preprocessing_pipeline.connect(\"document_embedder\", \"document_writer\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8fddbeb5665f26",
      "metadata": {
        "collapsed": false,
        "id": "c8fddbeb5665f26"
      },
      "source": [
        "Run the pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a1c47cb85342d86",
      "metadata": {
        "id": "7a1c47cb85342d86"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "preprocessing_pipeline.run(data = {\"document_joiner\": { \"documents\" : ready_list }})\n",
        "# preprocessing_pipeline.inputs()\n",
        "preprocessing_pipeline.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9eae0c4f1384253e",
      "metadata": {
        "collapsed": false,
        "id": "9eae0c4f1384253e"
      },
      "source": [
        "## Test RAG with Pinecone Document Store"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75a635b2d20f0142",
      "metadata": {
        "collapsed": false,
        "id": "75a635b2d20f0142"
      },
      "source": [
        "Restart the kernel and run the following code to test the RAG pipeline with the populated Pinecone Document Store."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58ebaae516e5343c",
      "metadata": {
        "collapsed": false,
        "id": "58ebaae516e5343c"
      },
      "source": [
        "Create pipeline to run a query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b38c8ef5aacfe0",
      "metadata": {
        "id": "1b38c8ef5aacfe0"
      },
      "outputs": [],
      "source": [
        "from haystack.components.embedders import SentenceTransformersDocumentEmbedder, SentenceTransformersTextEmbedder\n",
        "from haystack_integrations.components.retrievers.pinecone import PineconeEmbeddingRetriever\n",
        "from haystack.components.generators import OpenAIGenerator\n",
        "from haystack.components.builders.answer_builder import AnswerBuilder\n",
        "from haystack.components.builders.prompt_builder import PromptBuilder\n",
        "from haystack import Pipeline\n",
        "\n",
        "template = \"\"\"\n",
        "    Given these documents, answer the question.\\nDocuments:\n",
        "    {% for doc in documents %}\n",
        "        {{ doc.content }}\n",
        "    {% endfor %}\n",
        "\n",
        "    \\nQuestion: {{question}}\n",
        "    \\nAnswer:\n",
        "\"\"\"\n",
        "\n",
        "document_embedder = SentenceTransformersDocumentEmbedder(model=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "text_embedder = SentenceTransformersTextEmbedder(model=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "retriever = PineconeEmbeddingRetriever(document_store=document_store)\n",
        "generator = OpenAIGenerator()\n",
        "answer_builder = AnswerBuilder()\n",
        "prompt_builder = PromptBuilder(template=template)\n",
        "\n",
        "rag_pipeline = Pipeline()\n",
        "rag_pipeline.add_component(\"text_embedder\", text_embedder)\n",
        "rag_pipeline.add_component(\"retriever\", retriever)\n",
        "rag_pipeline.add_component(\"prompt_builder\", prompt_builder)\n",
        "rag_pipeline.add_component(\"llm\", generator)\n",
        "rag_pipeline.add_component(\"answer_builder\", answer_builder)\n",
        "\n",
        "rag_pipeline.connect(\"text_embedder.embedding\", \"retriever.query_embedding\")\n",
        "rag_pipeline.connect(\"retriever\", \"prompt_builder.documents\")\n",
        "rag_pipeline.connect(\"prompt_builder\", \"llm\")\n",
        "rag_pipeline.connect(\"llm.replies\", \"answer_builder.replies\")\n",
        "rag_pipeline.connect(\"retriever\", \"answer_builder.documents\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7c6c62a9fa033a1",
      "metadata": {
        "collapsed": false,
        "id": "f7c6c62a9fa033a1"
      },
      "source": [
        "Run the pipeline with a query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d066fdea1ddd46d3",
      "metadata": {
        "id": "d066fdea1ddd46d3"
      },
      "outputs": [],
      "source": [
        "# query = \"Generate full documentation of DataAnalyzer project\"\n",
        "query = \"Show me new changes from changelog\"\n",
        "result = rag_pipeline.run({\n",
        "    \"text_embedder\": {\"text\": query},\n",
        "    \"prompt_builder\": {\"question\": query},\n",
        "    \"answer_builder\": {\"query\": query}\n",
        "})\n",
        "\n",
        "print(result['answer_builder']['answers'][0].query)\n",
        "print(result['answer_builder']['answers'][0].data)\n",
        "print(result['answer_builder']['answers'][0].documents)\n",
        "\n",
        "with open(\"output.md\", \"w\") as f:\n",
        "    f.write(result['answer_builder']['answers'][0].data)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}