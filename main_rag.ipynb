{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# Wire RAG <a href=\"https://colab.research.google.com/github/appunite/Wire-RAG/blob/main/main_rag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "id": "780d3dac3fa3a702"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7d3909fd99fe64ed"
  },
  {
   "cell_type": "markdown",
   "source": "Install dependencies for colab",
   "metadata": {
    "collapsed": false
   },
   "id": "b4ef237387298e05"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "L5aAdbvCNIME",
   "metadata": {
    "id": "L5aAdbvCNIME"
   },
   "outputs": [],
   "source": [
    "!pip install haystack-ai pinecone-haystack sentence-transformers pinecone transformers\n",
    "!wget -P utils https://raw.githubusercontent.com/appunite/Wire-RAG/main/utils/url_scraper.py\n",
    "!wget -P utils https://raw.githubusercontent.com/appunite/Wire-RAG/main/utils/github_scraper.py"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Enter api keys",
   "id": "d2ec40ad9144d8d6"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "os.environ[\"PINECONE_API_KEY\"] = getpass.getpass(\"pinecone api key\")\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"open ai api key\")\n",
    "os.environ[\"GITHUB_API_TOKEN\"] = getpass.getpass(\"github api token (PAT)\")"
   ],
   "metadata": {
    "id": "hgvsByZlcsN7",
    "ExecuteTime": {
     "end_time": "2024-09-23T08:16:51.569319Z",
     "start_time": "2024-09-23T08:16:49.423772Z"
    }
   },
   "id": "hgvsByZlcsN7",
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "Or load keys from .env file"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e56187b91fd48f4c"
  },
  {
   "cell_type": "code",
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-01T13:59:57.580349Z",
     "start_time": "2024-10-01T13:59:57.569352Z"
    }
   },
   "id": "c4ff7e3293da16da",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "b36f5fc7",
   "metadata": {
    "id": "b36f5fc7"
   },
   "source": [
    "## Populate Pinecone Database"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Scrape URLs\n",
   "metadata": {
    "collapsed": false
   },
   "id": "f9debb5e3cf24851"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Whitelist: Allow any URL that begins with any element from the white_list.\\\n",
    "Blacklist: Block any URL that begins with any element from the black_list."
   ],
   "id": "507b685542fead85"
  },
  {
   "metadata": {
    "id": "393ea1e7",
    "ExecuteTime": {
     "end_time": "2024-10-01T08:00:34.188137Z",
     "start_time": "2024-10-01T08:00:13.067599Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import nest_asyncio\n",
    "from utils.url_scraper import start_scraping\n",
    "\n",
    "# Apply the nest_asyncio patch to allow nested event loops in Jupyter\n",
    "nest_asyncio.apply()\n",
    "\n",
    "starting_url = \"https://docs.wire.com\"\n",
    "depth_limit = 2\n",
    "\n",
    "filter_list = {\"white_list\": [\"https://docs.wire.com\"], \"black_list\": []}\n",
    "scraped_urls = await start_scraping(starting_url, depth_limit, filter_list)\n",
    "\n",
    "print(f\"Total URLs found: {len(scraped_urls)}\")"
   ],
   "id": "393ea1e7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total URLs found: 429\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "b5a20874",
   "metadata": {
    "id": "b5a20874"
   },
   "source": [
    "### Extract metadata and content"
   ]
  },
  {
   "cell_type": "code",
   "id": "8005a467",
   "metadata": {
    "id": "8005a467",
    "ExecuteTime": {
     "end_time": "2024-10-01T08:01:48.072118Z",
     "start_time": "2024-10-01T08:00:43.421241Z"
    }
   },
   "source": [
    "from utils.url_scraper import extract_content_and_metadata, DATE_FORMATS, DATE_PATTERNS\n",
    "\n",
    "scraped_urls_dict = []\n",
    "for u in scraped_urls:\n",
    "    scraped_urls_dict += extract_content_and_metadata(u, DATE_FORMATS, DATE_PATTERNS)\n",
    "print(len(scraped_urls_dict))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6679\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Scrape GitHub",
   "id": "7879deb1c0dd3bac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from utils.github_scraper import scrape_md_files\n",
    "\n",
    "md_dict = await scrape_md_files(org_name=\"wireapp\", api_key=os.getenv(\"GITHUB_API_TOKEN\"), repo_limit=None)\n",
    "print(len(md_dict))"
   ],
   "id": "d98a392a2e974513"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Save / Load .json",
   "id": "82d4fa18779e1dc6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import json\n",
    "\n",
    "# with open(\"./github_docs.json\", \"w\", encoding='utf-8') as json_file:\n",
    "#     json.dump(md_dict, json_file, ensure_ascii=False, indent=4)\n",
    "# \n",
    "# with open(\"./docs_wire.json\", \"w\", encoding='utf-8') as json_file:\n",
    "#     json.dump(scraped_urls_dict, json_file, ensure_ascii=False, indent=4)\n",
    "    \n",
    "with open(\"./github_docs.json\", 'r', encoding='utf-8') as json_file:\n",
    "    md_dict = json.load(json_file)\n",
    "print(len(md_dict), md_dict[0]['metadata'], sep='\\n')\n",
    "\n",
    "with open(\"./docs_wire.json\", 'r', encoding='utf-8') as json_file:\n",
    "    scraped_urls_dict = json.load(json_file)\n",
    "print(len(scraped_urls_dict), scraped_urls_dict[0]['metadata'], sep='\\n')"
   ],
   "id": "977e3247b21e728"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Populate database",
   "id": "e6e5448c382465a3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "To delete all records u need to `pip install \"pinecone[grpc]\"` and run the following code.",
   "id": "9ccfa24d598e7154"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Uncomment to delete all db records\n",
    "# import os\n",
    "# from pinecone import Pinecone\n",
    "# Pinecone(api_key=os.getenv(\"PINECONE_API_KEY\")).Index(\"wire-rag\").delete(delete_all=True, namespace='docs-wire')"
   ],
   "id": "e631c74c1a81b18b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Initialize Pinecone Document Store",
   "id": "dddf8e3581e624bc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from haystack.components.preprocessors import DocumentSplitter, DocumentCleaner\n",
    "from haystack.components.embedders import SentenceTransformersDocumentEmbedder\n",
    "from haystack.components.writers import DocumentWriter\n",
    "from haystack import Pipeline\n",
    "from haystack import Document\n",
    "from haystack_integrations.document_stores.pinecone import PineconeDocumentStore\n",
    "\n",
    "docs_wire_ds = PineconeDocumentStore(\n",
    "    index=\"wire-rag\",\n",
    "    namespace=\"docs-wire\",\n",
    "    dimension=384,\n",
    "    metric=\"cosine\",\n",
    "    spec={\"serverless\": {\"region\": \"us-east-1\", \"cloud\": \"aws\"}}\n",
    ")\n",
    "\n",
    "github_wireapp_ds = PineconeDocumentStore(\n",
    "    index=\"wire-rag\",\n",
    "    namespace=\"github-wireapp\",\n",
    "    dimension=384,\n",
    "    metric=\"cosine\",\n",
    "    spec={\"serverless\": {\"region\": \"us-east-1\", \"cloud\": \"aws\"}}\n",
    ")\n",
    "\n",
    "scraped_urls_documents = [Document(content=doc[\"content\"], meta=doc[\"metadata\"]) for doc in scraped_urls_dict]\n",
    "print(f\"Scraped URLs documents: {len(scraped_urls_documents)}\")\n",
    "\n",
    "github_documents = [Document(content=doc[\"content\"], meta=doc[\"metadata\"]) for doc in md_dict]\n",
    "print(f\"Github documents: {len(github_documents)}\")"
   ],
   "id": "ce2d96d606853561"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Create a pipelines to populate the Pinecone Document Store with both GitHub and docs.wire documents",
   "id": "5428894bb446f646"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# For all-MiniLM-L6-v2 default input text is 256 word pieces.\n",
    "splitter_gh = DocumentSplitter(split_by=\"word\", split_length=256, split_overlap=20)\n",
    "embedder_gh = SentenceTransformersDocumentEmbedder(model=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "writer_gh = DocumentWriter(github_wireapp_ds)\n",
    "\n",
    "pipeline_github = Pipeline()\n",
    "pipeline_github.add_component(instance=splitter_gh, name=\"splitter_gh\")\n",
    "pipeline_github.add_component(instance=embedder_gh, name=\"embedder_gh\")\n",
    "pipeline_github.add_component(instance=writer_gh, name=\"writer_gh\")\n",
    "\n",
    "pipeline_github.connect(\"splitter_gh\", \"embedder_gh\")\n",
    "pipeline_github.connect(\"embedder_gh\", \"writer_gh\")"
   ],
   "id": "3a7fb5fdda6e41d3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "cleaner_scraped = DocumentCleaner()\n",
    "# For all-MiniLM-L6-v2 default input text is 256 word pieces.\n",
    "splitter_scraped = DocumentSplitter(split_by=\"word\", split_length=256, split_overlap=20)\n",
    "embedder_scraped = SentenceTransformersDocumentEmbedder(model=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "writer_scraped = DocumentWriter(docs_wire_ds)\n",
    "\n",
    "pipeline_scraped = Pipeline()\n",
    "pipeline_scraped.add_component(instance=cleaner_scraped, name=\"cleaner_scraped\")\n",
    "pipeline_scraped.add_component(instance=splitter_scraped, name=\"splitter_scraped\")\n",
    "pipeline_scraped.add_component(instance=embedder_scraped, name=\"embedder_scraped\")\n",
    "pipeline_scraped.add_component(instance=writer_scraped, name=\"writer_scraped\")\n",
    "\n",
    "pipeline_scraped.connect(\"cleaner_scraped\", \"splitter_scraped\")\n",
    "pipeline_scraped.connect(\"splitter_scraped\", \"embedder_scraped\")\n",
    "pipeline_scraped.connect(\"embedder_scraped\", \"writer_scraped\")"
   ],
   "id": "70d8f0b61ea67a8e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Run the pipeline",
   "id": "eb1e3e1f7c4650b6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "pipeline_github.run(data = {\"splitter_gh\": { \"documents\" : github_documents }})\n",
    "pipeline_scraped.run(data = {\"cleaner_scraped\": { \"documents\" : scraped_urls_documents }})\n",
    "# preprocessing_pipeline.show()"
   ],
   "id": "814d143933ecc6e3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Test RAG with Pinecone Document Store",
   "id": "a5125088e60ad0d2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Restart the kernel and run the following code to test the RAG pipeline with the populated Pinecone Document Store.\\\n",
    "Create pipeline to run a query"
   ],
   "id": "4f516c7b1c07049c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from haystack.components.embedders import SentenceTransformersTextEmbedder\n",
    "from haystack_integrations.components.retrievers.pinecone import PineconeEmbeddingRetriever\n",
    "from haystack.components.generators import OpenAIGenerator\n",
    "from haystack.components.builders.answer_builder import AnswerBuilder\n",
    "from haystack.components.builders.prompt_builder import PromptBuilder\n",
    "from haystack.components.joiners.document_joiner import DocumentJoiner\n",
    "from haystack import Pipeline\n",
    "from haystack_integrations.document_stores.pinecone import PineconeDocumentStore\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "system_prompt = \"\"\"Task: Generate comprehensive documentation based on the provided documents. The documentation should capture all essential details and information without any information loss, including relevant code fragments from the files. Ensure that the output does not cut corners on tokens; generate as much content as possible within the limits of the provided documents. Do not reference or include information from any sources other than the given documents, do not rely on your knowledge.\n",
    "\n",
    "Requirements:\n",
    "\n",
    "1. Length: The documentation should be detailed and extensive, covering all aspects of the content provided in the documents.\n",
    "\n",
    "2. Structure:\n",
    "   - Title Page: Include a title that reflects the main theme of the documents.\n",
    "   - Introduction: Provide a complete and thorough overview of the content, purpose, and scope of the documentation. Explain the objectives of the documents and how the information will be structured.\n",
    "   - Main Sections: Divide the documentation into logical sections and subsections based on the themes and topics found in the documents. Each section should include:\n",
    "     - A clear, descriptive heading that reflects the content within the section.\n",
    "     - Detailed explanations of key concepts and ideas. Expand on each idea or concept with thorough descriptions, definitions, and context. Ensure the reader fully understands the material.\n",
    "     - Code fragments: Where applicable, include relevant code snippets from the documents. These should be properly formatted and explained in detail. For each code snippet, provide a clear explanation of its purpose, functionality, and how it fits into the overall system or process described in the document.\n",
    "     - Examples: Provide relevant examples where applicable, explaining them fully to ensure understanding.\n",
    "     - Conflict Resolution: If there are contradictory pieces of information across different documents, resolve them using the date provided at the beginning of each document, prioritizing the most recent information. If a documentâ€™s date is unknown and conflicts cannot be resolved, explicitly indicate which parts contradict one another, then clearly explain the contradiction. Mention all unresolved conflicts at the end of each section.\n",
    "\n",
    "3. Content Guidelines:\n",
    "   - Ensure that all information from the documents is included and accurately represented.\n",
    "   - The content should not just list bullet points but should be expanded into full sentences and paragraphs where necessary, with elaboration and explanations for all points.\n",
    "   - Use clear and logical transitions between sections to maintain the flow and coherence of the document.\n",
    "   - Use bullet points, tables, code fragments, or diagrams where appropriate to enhance understanding and readability, but always accompany these elements with in-depth explanations.\n",
    "\n",
    "4. Style: The writing style should be professional, formal, and suitable for an audience that may include technical experts, stakeholders, or general readers interested in the topic. Avoid jargon unless it is explained in detail.\n",
    "\n",
    "5. Specifics: Highlight any critical findings, data, or statistics that are present in the documents. For each significant point, explain its relevance and implications. Emphasize any unique or innovative aspects that stand out, and provide complete reasoning or analysis where needed.\n",
    "\n",
    "Output Format: Ensure that the entire documentation is generated in Markdown format without any additional text or formatting. The output should be ready to be saved directly into a markdown file.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "template = \"\"\"\n",
    "User Question: {{question}}\n",
    "Documents to Analyze:\n",
    "{% for doc in documents %}\n",
    "Date: {{doc.meta['date']}}\n",
    "Title: {{doc.meta['title']}} - {{doc.meta['headline']}}\n",
    "Content: \n",
    "{{doc.content}}\n",
    "{% endfor %}\"\"\"\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "github_wireapp_ds = PineconeDocumentStore(\n",
    "    index=\"wire-rag\",\n",
    "    namespace=\"github-wireapp\",\n",
    "    dimension=384,\n",
    "    metric=\"cosine\",\n",
    "    spec={\"serverless\": {\"region\": \"us-east-1\", \"cloud\": \"aws\"}}\n",
    ")\n",
    "\n",
    "docs_wire_ds = PineconeDocumentStore(\n",
    "    index=\"wire-rag\",\n",
    "    namespace=\"docs-wire\",\n",
    "    dimension=384,\n",
    "    metric=\"cosine\",\n",
    "    spec={\"serverless\": {\"region\": \"us-east-1\", \"cloud\": \"aws\"}}\n",
    ")\n",
    "\n",
    "text_embedder = SentenceTransformersTextEmbedder(model=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "retriever_gh = PineconeEmbeddingRetriever(document_store=github_wireapp_ds, top_k=25)\n",
    "retriever_docs_wire = PineconeEmbeddingRetriever(document_store=docs_wire_ds, top_k=25)\n",
    "joiner = DocumentJoiner(join_mode=\"concatenate\")\n",
    "prompt_builder = PromptBuilder(template=template)\n",
    "generator = OpenAIGenerator(model=\"gpt-4o-mini\", system_prompt=system_prompt) # \"gpt-4o-mini\" \"gpt-4o\" \"gpt-3.5-turbo\"\n",
    "answer_builder = AnswerBuilder()\n",
    "\n",
    "rag_pipeline = Pipeline()\n",
    "rag_pipeline.add_component(\"text_embedder\", text_embedder)\n",
    "rag_pipeline.add_component(\"retriever_gh\", retriever_gh)\n",
    "rag_pipeline.add_component(\"retriever_docs_wire\", retriever_docs_wire)\n",
    "rag_pipeline.add_component(\"joiner\", joiner)\n",
    "rag_pipeline.add_component(\"prompt_builder\", prompt_builder)\n",
    "rag_pipeline.add_component(\"generator\", generator)\n",
    "rag_pipeline.add_component(\"answer_builder\", answer_builder)\n",
    "\n",
    "rag_pipeline.connect(\"text_embedder.embedding\", \"retriever_gh.query_embedding\")\n",
    "rag_pipeline.connect(\"text_embedder.embedding\", \"retriever_docs_wire.query_embedding\")\n",
    "rag_pipeline.connect(\"retriever_docs_wire\", \"joiner\")\n",
    "rag_pipeline.connect(\"retriever_gh\", \"joiner\")\n",
    "\n",
    "rag_pipeline.connect(\"joiner\", \"prompt_builder.documents\")\n",
    "rag_pipeline.connect(\"joiner\", \"answer_builder.documents\")\n",
    "rag_pipeline.connect(\"prompt_builder\", \"generator\")\n",
    "rag_pipeline.connect(\"generator.replies\", \"answer_builder.replies\")\n",
    "\n",
    "with open(\"./pipeline.yml\", \"w\") as file:\n",
    "  rag_pipeline.dump(file)"
   ],
   "id": "d57a03567a4c79a8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Run the pipeline with a query",
   "id": "8d60b2c12101ca8f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "query = \"Generate full documentation about legal hold\"\n",
    "result = rag_pipeline.run({\n",
    "    \"text_embedder\": {\"text\": query},\n",
    "    \"prompt_builder\": {\"question\": query},\n",
    "    \"answer_builder\": {\"query\": query}\n",
    "})\n",
    "\n",
    "print(result['answer_builder']['answers'][0].query)\n",
    "print(result['answer_builder']['answers'][0].data)\n",
    "\n",
    "with open(\"./output.md\", \"w\") as f:\n",
    "    f.write(result['answer_builder']['answers'][0].data)"
   ],
   "id": "48b6e988492c52ff"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(f\"Finish Reason: {result['generator']['meta'][0]['finish_reason']}\\n\" \n",
    "      f\"Output Tokens: {result['generator']['meta'][0]['usage']['completion_tokens']}\\n\" \n",
    "      f\"Input Tokens: {result['generator']['meta'][0]['usage']['prompt_tokens']}\\n\")\n",
    "for i, doc in enumerate(result['answer_builder']['answers'][0].documents):\n",
    "    print(f\"{i + 1}. {doc.score} {doc.meta['title']} {doc.meta['headline']}\\n    {doc.to_dict()['url']}\")"
   ],
   "id": "1dff72fa83ce3450"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "for doc in result['answer_builder']['answers'][0].documents:\n",
    "    print(doc.meta, doc.content)"
   ],
   "id": "a388a2e3a9ee36bb"
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "include_colab_link": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
