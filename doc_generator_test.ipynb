{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Populate Pinecone Document Store with Test Case Documents"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ac9e9f500c62e007"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from haystack.components.writers import DocumentWriter\n",
    "from haystack.components.converters import MarkdownToDocument, PyPDFToDocument, TextFileToDocument\n",
    "from haystack.components.preprocessors import DocumentSplitter, DocumentCleaner\n",
    "from haystack.components.routers import FileTypeRouter\n",
    "from haystack.components.joiners import DocumentJoiner\n",
    "from haystack.components.embedders import SentenceTransformersDocumentEmbedder\n",
    "from haystack import Pipeline\n",
    "from haystack_integrations.document_stores.pinecone import PineconeDocumentStore\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "document_store = PineconeDocumentStore(\n",
    "\t\tindex=\"default\",\n",
    "\t\tnamespace=\"default\",\n",
    "\t\tdimension=384,\n",
    "  \tmetric=\"cosine\",\n",
    "  \tspec={\"serverless\": {\"region\": \"us-east-1\", \"cloud\": \"aws\"}}\n",
    ")\n",
    "\n",
    "file_type_router = FileTypeRouter(mime_types=[\"text/plain\", \"application/pdf\", \"text/markdown\"])\n",
    "text_file_converter = TextFileToDocument()\n",
    "markdown_converter = MarkdownToDocument()\n",
    "pdf_converter = PyPDFToDocument()\n",
    "document_joiner = DocumentJoiner()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-19T12:52:10.830231Z",
     "start_time": "2024-09-19T12:52:10.824097Z"
    }
   },
   "id": "a745131d322d28",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "document_cleaner = DocumentCleaner()\n",
    "document_splitter = DocumentSplitter(split_by=\"word\", split_length=150, split_overlap=50)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-19T12:52:10.838879Z",
     "start_time": "2024-09-19T12:52:10.831239Z"
    }
   },
   "id": "5142cfb8161abc1",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "document_embedder = SentenceTransformersDocumentEmbedder(model=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "document_writer = DocumentWriter(document_store)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-19T12:52:10.848553Z",
     "start_time": "2024-09-19T12:52:10.839885Z"
    }
   },
   "id": "52cabefb46937d22",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "<haystack.core.pipeline.pipeline.Pipeline object at 0x000001D5D76485F0>\n🚅 Components\n  - file_type_router: FileTypeRouter\n  - text_file_converter: TextFileToDocument\n  - markdown_converter: MarkdownToDocument\n  - pypdf_converter: PyPDFToDocument\n  - document_joiner: DocumentJoiner\n  - document_cleaner: DocumentCleaner\n  - document_splitter: DocumentSplitter\n  - document_embedder: SentenceTransformersDocumentEmbedder\n  - document_writer: DocumentWriter\n🛤️ Connections\n  - file_type_router.text/plain -> text_file_converter.sources (List[Path])\n  - file_type_router.application/pdf -> pypdf_converter.sources (List[Path])\n  - file_type_router.text/markdown -> markdown_converter.sources (List[Path])\n  - text_file_converter.documents -> document_joiner.documents (List[Document])\n  - markdown_converter.documents -> document_joiner.documents (List[Document])\n  - pypdf_converter.documents -> document_joiner.documents (List[Document])\n  - document_joiner.documents -> document_cleaner.documents (List[Document])\n  - document_cleaner.documents -> document_splitter.documents (List[Document])\n  - document_splitter.documents -> document_embedder.documents (List[Document])\n  - document_embedder.documents -> document_writer.documents (List[Document])"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessing_pipeline = Pipeline()\n",
    "preprocessing_pipeline.add_component(instance=file_type_router, name=\"file_type_router\")\n",
    "preprocessing_pipeline.add_component(instance=text_file_converter, name=\"text_file_converter\")\n",
    "preprocessing_pipeline.add_component(instance=markdown_converter, name=\"markdown_converter\")\n",
    "preprocessing_pipeline.add_component(instance=pdf_converter, name=\"pypdf_converter\")\n",
    "preprocessing_pipeline.add_component(instance=document_joiner, name=\"document_joiner\")\n",
    "preprocessing_pipeline.add_component(instance=document_cleaner, name=\"document_cleaner\")\n",
    "preprocessing_pipeline.add_component(instance=document_splitter, name=\"document_splitter\")\n",
    "preprocessing_pipeline.add_component(instance=document_embedder, name=\"document_embedder\")\n",
    "preprocessing_pipeline.add_component(instance=document_writer, name=\"document_writer\")\n",
    "\n",
    "preprocessing_pipeline.connect(\"file_type_router.text/plain\", \"text_file_converter.sources\")\n",
    "preprocessing_pipeline.connect(\"file_type_router.application/pdf\", \"pypdf_converter.sources\")\n",
    "preprocessing_pipeline.connect(\"file_type_router.text/markdown\", \"markdown_converter.sources\")\n",
    "preprocessing_pipeline.connect(\"text_file_converter\", \"document_joiner\")\n",
    "preprocessing_pipeline.connect(\"pypdf_converter\", \"document_joiner\")\n",
    "preprocessing_pipeline.connect(\"markdown_converter\", \"document_joiner\")\n",
    "preprocessing_pipeline.connect(\"document_joiner\", \"document_cleaner\")\n",
    "preprocessing_pipeline.connect(\"document_cleaner\", \"document_splitter\")\n",
    "preprocessing_pipeline.connect(\"document_splitter\", \"document_embedder\")\n",
    "preprocessing_pipeline.connect(\"document_embedder\", \"document_writer\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-19T12:52:10.944932Z",
     "start_time": "2024-09-19T12:52:10.849557Z"
    }
   },
   "id": "f0df56147ec0fad8",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\filip\\Desktop\\venvs\\wireenv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Converting markdown files to Documents: 100%|██████████| 8/8 [00:00<00:00, 2000.38it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": "Batches:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "89b9ae5744d141ab8c5097d477752c1b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Document 136bcc65efa842bf4dab45cc51c0b89df20d582eb2c965c08134364c020bc1ac has metadata fields with unsupported types: ['_split_overlap']. Only str, int, bool, and List[str] are supported. The values of these fields will be discarded.\n",
      "Document a6c0e5304630bcd37e2dad1fff7ab046d0d9965e26384b08d134d08a3bf009cd has metadata fields with unsupported types: ['_split_overlap']. Only str, int, bool, and List[str] are supported. The values of these fields will be discarded.\n"
     ]
    },
    {
     "data": {
      "text/plain": "Upserted vectors:   0%|          | 0/25 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e96629cc1b334092bcd25fe48fdf27fa"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "{'document_writer': {'documents_written': 25}}"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "preprocessing_pipeline.run({\"file_type_router\": {\"sources\": list(Path(Path('./Data/Test_Case')).glob(\"**/*\"))}})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-19T12:52:14.979509Z",
     "start_time": "2024-09-19T12:52:10.944932Z"
    }
   },
   "id": "7a1c47cb85342d86",
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test RAG with Pinecone Document Store"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9eae0c4f1384253e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Restart the kernel and run the following code to test the RAG pipeline with the populated Pinecone Document Store."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "75a635b2d20f0142"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from haystack_integrations.document_stores.pinecone import PineconeDocumentStore\n",
    "\n",
    "# Load OpenAI API key from .env file\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "document_store = PineconeDocumentStore(\n",
    "    index=\"default\",\n",
    "    namespace=\"default\",\n",
    "    dimension=384,\n",
    "  \tmetric=\"cosine\",\n",
    "  \tspec={\"serverless\": {\"region\": \"us-east-1\", \"cloud\": \"aws\"}}\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-19T14:07:12.205476Z",
     "start_time": "2024-09-19T14:07:08.698565Z"
    }
   },
   "id": "6bfc12597e46077",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "<haystack.core.pipeline.pipeline.Pipeline object at 0x000002A2096FE540>\n🚅 Components\n  - text_embedder: SentenceTransformersTextEmbedder\n  - retriever: PineconeEmbeddingRetriever\n  - prompt_builder: PromptBuilder\n  - llm: OpenAIGenerator\n  - answer_builder: AnswerBuilder\n🛤️ Connections\n  - text_embedder.embedding -> retriever.query_embedding (List[float])\n  - retriever.documents -> prompt_builder.documents (List[Document])\n  - retriever.documents -> answer_builder.documents (List[Document])\n  - prompt_builder.prompt -> llm.prompt (str)\n  - llm.replies -> answer_builder.replies (List[str])"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from haystack.components.embedders import SentenceTransformersDocumentEmbedder, SentenceTransformersTextEmbedder\n",
    "from haystack_integrations.components.retrievers.pinecone import PineconeEmbeddingRetriever\n",
    "from haystack.components.generators import OpenAIGenerator\n",
    "from haystack.components.builders.answer_builder import AnswerBuilder\n",
    "from haystack.components.builders.prompt_builder import PromptBuilder\n",
    "from haystack import Pipeline\n",
    "\n",
    "template = \"\"\"\n",
    "    Given these documents, answer the question.\\nDocuments:\n",
    "    {% for doc in documents %}\n",
    "        {{ doc.content }}\n",
    "    {% endfor %}\n",
    "\n",
    "    \\nQuestion: {{question}}\n",
    "    \\nAnswer:\n",
    "\"\"\"\n",
    "\n",
    "document_embedder = SentenceTransformersDocumentEmbedder(model=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "text_embedder = SentenceTransformersTextEmbedder(model=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "retriever = PineconeEmbeddingRetriever(document_store=document_store)\n",
    "generator = OpenAIGenerator()\n",
    "answer_builder = AnswerBuilder()\n",
    "prompt_builder = PromptBuilder(template=template)\n",
    "\n",
    "rag_pipeline = Pipeline()\n",
    "rag_pipeline.add_component(\"text_embedder\", text_embedder)\n",
    "rag_pipeline.add_component(\"retriever\", retriever)\n",
    "rag_pipeline.add_component(\"prompt_builder\", prompt_builder)\n",
    "rag_pipeline.add_component(\"llm\", generator)\n",
    "rag_pipeline.add_component(\"answer_builder\", answer_builder)\n",
    "\n",
    "rag_pipeline.connect(\"text_embedder.embedding\", \"retriever.query_embedding\")\n",
    "rag_pipeline.connect(\"retriever\", \"prompt_builder.documents\")\n",
    "rag_pipeline.connect(\"prompt_builder\", \"llm\")\n",
    "rag_pipeline.connect(\"llm.replies\", \"answer_builder.replies\")\n",
    "rag_pipeline.connect(\"retriever\", \"answer_builder.documents\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-19T14:07:14.047251Z",
     "start_time": "2024-09-19T14:07:14.040423Z"
    }
   },
   "id": "1b38c8ef5aacfe0",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\filip\\Desktop\\venvs\\wireenv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "Batches:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7b15e30aad3546ca89c243e1ad5ae3aa"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate full documentation of DataAnalyzer project\n",
      "**DataAnalyzer Project Documentation**\n",
      "\n",
      "**Module Name:** DataAnalyzer\n",
      "**Version:** 2.5.0\n",
      "**Author:** Tech Innovations\n",
      "\n",
      "**Overview:**\n",
      "The DataAnalyzer project provides a fast, optimized approach to data analysis, suitable for personal and academic projects. It offers functions for processing datasets and generating basic statistics and visualizations. This project is designed for small datasets and aims to simplify data analysis tasks.\n",
      "\n",
      "**Functions:**\n",
      "- `process_data(dataset: list) -> dict`: This function processes the dataset and returns basic statistics such as mean, mode, and variance.\n",
      "- `graph_data(dataset: list, chart: str = 'scatter') -> None`: This function generates a chart based on the dataset, with support for chart types like 'scatter', 'heatmap', and 'pie'.\n",
      "\n",
      "**Dependencies:**\n",
      "- Python 3.6+\n",
      "- pandas\n",
      "- seaborn\n",
      "\n",
      "**Installation:**\n",
      "Install the DataAnalyzer package via pip:\n",
      "```\n",
      "pip install analyzer-package\n",
      "```\n",
      "\n",
      "**How to Use:**\n",
      "1. Import the module: \n",
      "```python\n",
      "from DataAnalyzer import process_data, graph_data\n",
      "```\n",
      "2. Call the functions with the dataset as demonstrated in the examples.\n",
      "\n",
      "**Changes and Updates:**\n",
      "- The `process_data` function has replaced the `analyze_data` function for more comprehensive data processing.\n",
      "- The default chart type for `graph_data` has been changed to 'scatter', with added support for 'heatmap' and 'pie'.\n",
      "- Improved performance for handling small datasets.\n",
      "\n",
      "This project aims to simplify data analysis tasks for small datasets and provide users with essential statistical insights and visualizations. By following the installation and usage instructions, users can effectively utilize the DataAnalyzer package for their data analysis needs.\n",
      "[Document(id=5858b0617db9c1b474fa484f0a3bec8317fded4cfcdae84552bbee3249faf4d0, content: 'Code Documentation for DataAnalyzer.py\n",
      "Module Name: DataAnalyzer\n",
      "Version: 1.0.0\n",
      "Author: Tech Solutio...', meta: {'_split_overlap': [], 'file_path': 'Data\\\\Test_Case\\\\1.pdf', 'page_number': 1, 'source_id': '1416f0a216fd69c6e9a9028316744a042dc4663e8c0873b02eca4661feb987a2', 'split_id': 0, 'split_idx_start': 0}, score: 0.507769406, embedding: vector of size 384), Document(id=29a7dae24a4df16e8953fc75558c569e82cd47e3c6919d810fe9e81d12287724, content: 'DataAnalyzer Version: 2.5.0\n",
      "Author: Tech Innovations Overview The DataAnalyzer package provides a fa...', meta: {'_split_overlap': [], 'file_path': 'Data\\\\Test_Case\\\\1.md', 'page_number': 1, 'source_id': 'fa6b471b2169c5cf6b138e74e2f334fba71150573d6d513d052a67560a947eac', 'split_id': 0, 'split_idx_start': 0}, score: 0.46718663, embedding: vector of size 384), Document(id=09f41e90e86ba0d5f2ea54e6307656d06620c27481d9ab058bccacc72152c411, content: 'Meeting notes - The function `process_data` replaces `analyze_data`.\n",
      "- Default chart type for `graph...', meta: {'_split_overlap': [], 'file_path': 'Data\\\\Test_Case\\\\1.txt', 'page_number': 1, 'source_id': '49d7075b862b3ece8b6f123d4fd9c7a980ddfd6a507f7f470630e9f5cd2116d3', 'split_id': 0, 'split_idx_start': 0}, score: 0.413367271, embedding: vector of size 384), Document(id=88784b322a1bc9eb5bdac32739fea8e3ec38a0db2ab933759c93980e113f61e1, content: '# Patch Notes - reverse_string was renamed to analyze_text.\n",
      "- analyze_text now performs text analysi...', meta: {'_split_overlap': [], 'file_path': 'Data\\\\Test_Case\\\\5.txt', 'page_number': 1, 'source_id': '242e8ac8c27e41a76f07befe962ce4cbee2d6f5e503502927bc4126246e9e90e', 'split_id': 0, 'split_idx_start': 0}, score: 0.32659027, embedding: vector of size 384), Document(id=3aca361c8962d6556c966b5d8576d91034778781053db20e93abd0f2a52fd7d6, content: 'README Project Description This Ruby script is designed for text analysis, not basic string reversal...', meta: {'_split_overlap': [], 'file_path': 'Data\\\\Test_Case\\\\5.md', 'page_number': 1, 'source_id': '8fc343842065a5149f581ff978730fe914a91bb430ec0479c23d31e716217cfc', 'split_id': 0, 'split_idx_start': 0}, score: 0.285484016, embedding: vector of size 384), Document(id=4512a83362922d5f35826754e5a79ec748198457d02fa01950f72d2ce953ed2a, content: '# Python Code Documentation\n",
      "## Overview\n",
      "This document outlines the structure of a Python program des...', meta: {'_split_overlap': [], 'file_path': 'Data\\\\Test_Case\\\\3.pdf', 'page_number': 1, 'source_id': 'a388a2b8d91d07051ee48069f9a28f48f16e0729876589b8154584cfb60abeb7', 'split_id': 0, 'split_idx_start': 0}, score: 0.194724947, embedding: vector of size 384), Document(id=43d89de410abee85f1612ac4cb5847334e4285d77ca9cdc14f6ea232f676b10b, content: 'README Project Description This Python script is an interactive command-line tool designed for proce...', meta: {'_split_overlap': [], 'file_path': 'Data\\\\Test_Case\\\\3.md', 'page_number': 1, 'source_id': 'f19225d52b2230e510de7cb747efed97f2d1335dfd2ae2a017f0c3c969213f5c', 'split_id': 0, 'split_idx_start': 0}, score: 0.184688523, embedding: vector of size 384), Document(id=317094e95a461034dab2952073d52e1365e624a8cd11948f5f150fbbe51980ae, content: 'README Project Description This Java project is designed to demonstrate basic input/output handling....', meta: {'_split_overlap': [], 'file_path': 'Data\\\\Test_Case\\\\2.md', 'page_number': 1, 'source_id': 'ebdee72975da0e6d66faaf7ab0c3e275cf59e74438c5a7461c3692734ca77596', 'split_id': 0, 'split_idx_start': 0}, score: 0.168476269, embedding: vector of size 384), Document(id=5bdb1283969d71fda11f1b094bb9419b462a0784e0dcabe5ba095c820e282068, content: '# Patch Notes - Chat functionality replaced with file transfer.\n",
      "- Message handling functions were ch...', meta: {'_split_overlap': [], 'file_path': 'Data\\\\Test_Case\\\\7.txt', 'page_number': 1, 'source_id': 'ee0e31625150eda2c086e5f28f6030c85733c825a2d3f5622f418bde57f005e3', 'split_id': 0, 'split_idx_start': 0}, score: 0.166382805, embedding: vector of size 384), Document(id=45f2e3d9ca3beffe1ba8931fcc3f21a28079adad00d3773b0d70c92bbc1c0585, content: '# Patch Notes - substraction funtion added\n",
      "- helperFunction output format is changed', meta: {'_split_overlap': [], 'file_path': 'Data\\\\Test_Case\\\\4.txt', 'page_number': 1, 'source_id': '979196cb4a7f55e85d2bfefb6acbdf13b7c17b1661d84575c9d6bcc76a010221', 'split_id': 0, 'split_idx_start': 0}, score: 0.161468595, embedding: vector of size 384)]\n"
     ]
    }
   ],
   "source": [
    "query = \"Generate full documentation of DataAnalyzer project\"\n",
    "result = rag_pipeline.run({\n",
    "    \"text_embedder\": {\"text\": query},\n",
    "    \"prompt_builder\": {\"question\": query},\n",
    "    \"answer_builder\": {\"query\": query}\n",
    "})\n",
    "\n",
    "print(result['answer_builder']['answers'][0].query)\n",
    "print(result['answer_builder']['answers'][0].data)\n",
    "print(result['answer_builder']['answers'][0].documents)\n",
    "\n",
    "with open(\"./Data/Outputs/output.md\", \"w\") as f:\n",
    "    f.write(result['answer_builder']['answers'][0].data)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-19T14:07:25.022820Z",
     "start_time": "2024-09-19T14:07:16.731648Z"
    }
   },
   "id": "d066fdea1ddd46d3",
   "execution_count": 3
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
